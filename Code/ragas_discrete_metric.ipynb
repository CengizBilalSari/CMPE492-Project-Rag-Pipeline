{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad1207dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from ragas import experiment,EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import DiscreteMetric\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14cfab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"user_input\": \"What is Ragas 0.3?\", \n",
    "        \"reference\": \"Ragas 0.3 is a library for evaluating LLM applications.\",\n",
    "        \"response\": \"Ragas 0.3 is a tool used to evaluate LLM apps.\" # Should Pass\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"How to install Ragas?\", \n",
    "        \"reference\": \"install with pip using ragas[examples]\",\n",
    "        \"response\": \"You can install it using npm install .\" # Should Fail (wrong package manager)\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"What are the main features of Ragas?\", \n",
    "        \"reference\": \"organised around - experiments - datasets - metrics.\",\n",
    "        \"response\": \"It features experiments, datasets, and evaluation metrics.\" # Should Pass\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame(samples)\n",
    "dataset = EvaluationDataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa303b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\" \n",
    ")\n",
    "\n",
    "llm = llm_factory(\"llama3.2:3b\", provider=\"openai\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a8ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_metric_1 = DiscreteMetric(\n",
    "    name=\"correctness\",\n",
    "    prompt=\"\"\"\n",
    "    Check if the response contains the key points mentioned in the grading notes.\n",
    "    \n",
    "    Grading Notes: {grading_notes}\n",
    "    Response: {response}\n",
    "    \n",
    "    You must output your response in JSON format with the following keys:\n",
    "    1. \"reason\": A brief explanation of why the response passes or fails.\n",
    "    2. \"value\": Either \"pass\" or \"fail\".\n",
    "    \n",
    "    JSON Output:\n",
    "    \"\"\",\n",
    "    allowed_values=[\"pass\", \"fail\"],\n",
    ")\n",
    "my_metric_2 = DiscreteMetric(\n",
    "    name=\"correctness\",\n",
    "    prompt=\"\"\"\n",
    "    Check if the response accurately reflects the information in the grading notes.\n",
    "    \n",
    "    Grading Notes: {grading_notes}\n",
    "    Response: {response}\n",
    "    \n",
    "    Instructions:\n",
    "    - Focus on the SEMANTIC meaning. \n",
    "    - Synonyms (e.g., 'dashboard' instead of 'panel') should be accepted as 'pass'.\n",
    "    - Output JSON with \"reason\" and \"value\" ('pass' or 'fail').\n",
    "    \"\"\",\n",
    "    allowed_values=[\"pass\", \"fail\"],\n",
    ")\n",
    "my_metric_3 = DiscreteMetric(\n",
    "    name=\"correctness\",\n",
    "    prompt=\"\"\"\n",
    "    Role: You are a strict semantic evaluator.\n",
    "    Task: Compare the 'Response' against the 'Grading Notes'.\n",
    "    \n",
    "    Grading Logic:\n",
    "    - PASS if the meaning is the same, even if different words are used (e.g., synonyms).\n",
    "    - FAIL if there is a technical contradiction, incorrect command, or missing core fact.\n",
    "\n",
    "    Example A (Semantic Match):\n",
    "    Grading Notes: \"The software is compatible with macOS.\"\n",
    "    Response: \"It runs on Apple computers.\"\n",
    "    Result: {{\"reason\": \"Apple computers run macOS, so the meaning is preserved.\", \"value\": \"pass\"}}\n",
    "\n",
    "    Example B (Technical Error):\n",
    "    Grading Notes: \"Use the 'git push' command to upload code.\"\n",
    "    Response: \"Run 'git pull' to send your changes.\"\n",
    "    Result: {{\"reason\": \"The response suggests 'pull' (download) instead of 'push' (upload).\", \"value\": \"fail\"}}\n",
    "\n",
    "    Current Task:\n",
    "    Grading Notes: {grading_notes}\n",
    "    Response: {response}\n",
    "\n",
    "    Return the result in valid JSON format.\n",
    "    \"\"\",\n",
    "    allowed_values=[\"pass\", \"fail\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78863382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 3/3 [00:13<00:00,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'What is Ragas 0.3?', 'response': 'Ragas 0.3 is a tool used to evaluate LLM apps.', 'score': 'pass', 'reason': 'Although both responses mention Ragas 0.3, they use different terminology (library vs. tool), which may lead to confusion. However, the core meaning of evaluating LLM applications remains preserved.', 'reference': 'Ragas 0.3 is a library for evaluating LLM applications.'} \n",
      "\n",
      "{'user_input': 'How to install Ragas?', 'response': 'You can install it using npm install .', 'score': 'pass', 'reason': \"npm install is not a standard way to install Python packages, but 'install' and 'pip' are synonyms. The meaning of the response is preserved.\", 'reference': 'install with pip using ragas[examples]'} \n",
      "\n",
      "{'user_input': 'What are the main features of Ragas?', 'response': 'It features experiments, datasets, and evaluation metrics.', 'score': 'fail', 'reason': \"The response does not mention 'evaluation' as a core metric, which is an essential part of grading notes. However, it still conveys the same meaning by mentioning experiments, datasets, and metrics.\", 'reference': 'organised around - experiments - datasets - metrics.'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# jupyter.debugJustMyCode: make it unselected for enabling going to base files for debug\n",
    "# It is in \"ctrl + , \" , \"jupyter.debugJustMyCode\" \n",
    "from  ragas.backends import LocalCSVBackend # to give ragas a way to save the results\n",
    "@experiment(\n",
    "    name_prefix=\"ragas_discrete_metric_prompt_3_test\",\n",
    "    backend= LocalCSVBackend(root_dir=\".\")\n",
    ")\n",
    "async def run_rag_evaluation(row):\n",
    "    # access the specific 'response' and 'reference' defined above\n",
    "    # it will directly gives our prompt in my_metric to llm with grading notes and response \n",
    "    score = await my_metric_3.ascore(\n",
    "        llm=llm,\n",
    "        response=row.response,\n",
    "        grading_notes=row.reference\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"user_input\": row.user_input,\n",
    "        \"response\": row.response,\n",
    "        \"score\": score.value,\n",
    "        \"reason\": score.reason,\n",
    "        \"reference\": row.reference\n",
    "    }\n",
    "results = await run_rag_evaluation.arun(dataset=dataset)\n",
    "for result in results:\n",
    "    print(result,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cde2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv\\Lib\\site-packages\\instructor\\providers\\gemini\\client.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n",
      "C:\\Users\\Cengizhan\\AppData\\Local\\Temp\\ipykernel_9432\\1200223224.py:10: DeprecationWarning: Importing AnswerCorrectness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerCorrectness\n",
      "  from ragas.metrics import AnswerCorrectness, SemanticSimilarity\n",
      "C:\\Users\\Cengizhan\\AppData\\Local\\Temp\\ipykernel_9432\\1200223224.py:10: DeprecationWarning: Importing SemanticSimilarity from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import SemanticSimilarity\n",
      "  from ragas.metrics import AnswerCorrectness, SemanticSimilarity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ragas core imports\n",
    "from ragas import experiment, EvaluationDataset\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import LlamaIndexEmbeddingsWrapper # Or Langchain wrapper\n",
    "from ragas.metrics import AnswerCorrectness, SemanticSimilarity\n",
    "\n",
    "# Langchain HuggingFace (The actual implementation)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# 1. Initialize LLM (Ollama - Llama 3.2)\n",
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "llm = llm_factory(\"llama3.2:3b\", provider=\"openai\", client=client)\n",
    "\n",
    "# 2. Initialize Embeddings correctly\n",
    "# This uses the sentence-transformers library under the hood\n",
    "lc_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063897c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Numeric Metrics\n",
    "# Pass the langchain embeddings directly or wrap them if needed\n",
    "correctness_metric = AnswerCorrectness(llm=llm, embeddings=lc_embeddings)\n",
    "similarity_metric = SemanticSimilarity(embeddings=lc_embeddings)\n",
    "\n",
    "# 4. Create your test dataset\n",
    "samples = [\n",
    "    {\n",
    "        \"user_input\": \"What is Ragas 0.3?\", \n",
    "        \"reference\": \"Ragas 0.3 is a library for evaluating LLM applications.\",\n",
    "        \"response\": \"Ragas 0.3 is a tool used to evaluate LLM apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"How to install Ragas?\", \n",
    "        \"reference\": \"install from source - install from pip using ragas[examples]\",\n",
    "        \"response\": \"You can install it using npm install ragas.\"\n",
    "    }\n",
    "]\n",
    "dataset = EvaluationDataset.from_pandas(pd.DataFrame(samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daaf2852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cengizhan\\AppData\\Local\\Temp\\ipykernel_9432\\2355046309.py:20: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(lc_embeddings)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Collections metrics only support modern embeddings. Found: LangchainEmbeddingsWrapper. Use: embedding_factory('openai', model='text-embedding-ada-002', client=openai_client, interface='modern')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 3. Define Metrics\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Pass the wrapped embeddings directly to AnswerCorrectness. \u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# It will internally handle the similarity calculation.\u001b[39;00m\n\u001b[32m     25\u001b[39m correctness_metric = AnswerCorrectness(\n\u001b[32m     26\u001b[39m     llm=llm, \n\u001b[32m     27\u001b[39m     embeddings=ragas_embeddings\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m similarity_metric = \u001b[43mSemanticSimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mragas_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 5. Define the Experiment\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@experiment\u001b[39m()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_numeric_evaluation\u001b[39m(row):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv\\Lib\\site-packages\\ragas\\metrics\\collections\\_semantic_similarity.py:67\u001b[39m, in \u001b[36mSemanticSimilarity.__init__\u001b[39m\u001b[34m(self, embeddings, name, threshold, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.embeddings = embeddings\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.threshold = threshold\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv\\Lib\\site-packages\\ragas\\metrics\\collections\\base.py:46\u001b[39m, in \u001b[36mBaseMetric.__init__\u001b[39m\u001b[34m(self, name, allowed_values, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_llm()\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv\\Lib\\site-packages\\ragas\\metrics\\collections\\base.py:128\u001b[39m, in \u001b[36mBaseMetric._validate_embeddings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m embeddings = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, BaseRagasEmbedding):\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    129\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCollections metrics only support modern embeddings. Found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(embeddings).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUse: embedding_factory(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext-embedding-ada-002\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, client=openai_client, interface=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Collections metrics only support modern embeddings. Found: LangchainEmbeddingsWrapper. Use: embedding_factory('openai', model='text-embedding-ada-002', client=openai_client, interface='modern')"
     ]
    }
   ],
   "source": [
    "# 1. Corrected Imports\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from ragas import experiment, EvaluationDataset\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# Note: SemanticSimilarity and AnswerCorrectness are in collections, \n",
    "# but AnswerSimilarity might not be exported there in your version.\n",
    "from ragas.metrics.collections import AnswerCorrectness, SemanticSimilarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas.backends import LocalCSVBackend\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2. Initialize Embeddings with Wrapper\n",
    "lc_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(lc_embeddings)\n",
    "\n",
    "# 3. Define Metrics\n",
    "# Pass the wrapped embeddings directly to AnswerCorrectness. \n",
    "# It will internally handle the similarity calculation.\n",
    "correctness_metric = AnswerCorrectness(\n",
    "    llm=llm, \n",
    "    embeddings=ragas_embeddings\n",
    ")\n",
    "similarity_metric = SemanticSimilarity(embeddings=ragas_embeddings)\n",
    "\n",
    "# 5. Define the Experiment\n",
    "@experiment()\n",
    "async def run_numeric_evaluation(row):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=row.user_input,\n",
    "        response=row.response,\n",
    "        reference=row.reference\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics for the custom experiment loop\n",
    "    await correctness_metric.init()\n",
    "    await similarity_metric.init()\n",
    "    \n",
    "    score_correctness = await correctness_metric.single_turn_ascore(sample)\n",
    "    score_similarity = await similarity_metric.single_turn_ascore(sample)\n",
    "\n",
    "    return {\n",
    "        \"user_input\": row.user_input,\n",
    "        \"response\": row.response,\n",
    "        \"correctness_score\": score_correctness,\n",
    "        \"similarity_score\": score_similarity,\n",
    "        \"reference\": row.reference\n",
    "    }\n",
    "\n",
    "# 6. Run the evaluation\n",
    "async def main():\n",
    "    results = await run_numeric_evaluation.arun(\n",
    "        dataset=dataset, \n",
    "        name=\"numeric_test_run\",\n",
    "        backend=LocalCSVBackend(root_dir=\".\")\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    df_results = results.to_pandas()\n",
    "    print(df_results.head())\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602b944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
