{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faithfullness  will be used in here.\n",
    "\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from ragas import experiment, EvaluationDataset\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import HuggingFaceEmbeddings \n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.backends import LocalCSVBackend\n",
    "from ragas.metrics.collections import (\n",
    "    Faithfulness     \n",
    ")\n",
    "from ragas.metrics.collections.faithfulness import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09de4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "llm = llm_factory(\"qwen2.5:3b\", provider=\"openai\", client=client)\n",
    "#embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1bb2b",
   "metadata": {},
   "source": [
    "###### Since ragas does not provide traces and reasons in **ascore** method and **MetricResult class**, I needed to  rewrite them  with inheriting original class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa4d323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.result import MetricResult\n",
    "\n",
    "class DetailedMetricResult(MetricResult):\n",
    "    def __repr__(self):\n",
    "        base_repr = super().__repr__()\n",
    "        return f\"{base_repr[:-1]}, traces={self.traces})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Value: {self.value}\\n\"\n",
    "                f\"Reason: {self.reason}\\n\"\n",
    "                f\"Traces: {self.traces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00175b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.result import MetricResult\n",
    "from typing import List\n",
    "\n",
    "class TraceableFaithfulness(Faithfulness):\n",
    "    async def ascore(\n",
    "        self, user_input: str, response: str, retrieved_contexts: List[str]\n",
    "    ) -> DetailedMetricResult:\n",
    "        # Input validation\n",
    "        if not response:\n",
    "            raise ValueError(\n",
    "                \"response is missing. Please add response to the test sample.\"\n",
    "            )\n",
    "        if not user_input:\n",
    "            raise ValueError(\n",
    "                \"user_input is missing. Please add user_input to the test sample.\"\n",
    "            )\n",
    "        if not retrieved_contexts:\n",
    "            raise ValueError(\n",
    "                \"retrieved_contexts is missing. Please add retrieved_contexts to the test sample.\"\n",
    "            )\n",
    "\n",
    "        #Break response into atomic statements\n",
    "        statements = await self._create_statements(user_input, response)\n",
    "\n",
    "        if not statements:\n",
    "            # No statements generated - return NaN like legacy\n",
    "            return MetricResult(value=float(\"nan\"))\n",
    "\n",
    "        #Join all contexts and evaluate statements against them\n",
    "        context_str = \"\\n\".join(retrieved_contexts)\n",
    "        verdicts = await self._create_verdicts(statements, context_str)\n",
    "\n",
    "        #Compute faithfulness score\n",
    "        score = self._compute_score(verdicts)\n",
    "        \n",
    "        statements=[v.statement for v in verdicts.statements]\n",
    "        reasons = \", \".join([v.reason for v in verdicts.statements])\n",
    "\n",
    "        return DetailedMetricResult(\n",
    "            value=float(score),\n",
    "            reason= reasons,\n",
    "            traces={\n",
    "                \"input\": {\"statements\": statements},\n",
    "                \"output\": {\"verdicts\": [v.verdict for v in verdicts.statements]}\n",
    "            }\n",
    "        )\n",
    "\n",
    "faithfulness_metric = TraceableFaithfulness(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88934935",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"user_input\": \"What is Ragas 0.3?\", \n",
    "        \"retrieved_contexts\": [\"Ragas 0.3 is an evaluation framework for RAG pipelines.\"],\n",
    "        \"response\": \"Ragas 0.3 is a tool to evaluate LLM applications.\",\n",
    "        \"reference\": \"Ragas 0.3 is a library for evaluating LLM applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"How do I install Ragas?\", \n",
    "        \"retrieved_contexts\": [\"To install ragas, run pip install ragas.\"],\n",
    "        \"response\": \"The weather is quite nice for a walk today.\", # low \"Relevance\"\n",
    "        \"reference\": \"Install using pip install ragas.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = EvaluationDataset.from_pandas(pd.DataFrame(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9cffc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 2/2 [00:51<00:00, 25.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results\n",
      "                user_input  faithfulness_score  \\\n",
      "0       What is Ragas 0.3?                 0.0   \n",
      "1  How do I install Ragas?                 0.0   \n",
      "\n",
      "                                           reasoning  \\\n",
      "0  The context only mentions that Ragas 0.3 is an...   \n",
      "1  There is no mention of the weather in the cont...   \n",
      "\n",
      "                                          statements verdicts  \n",
      "0  Ragas 0.3 is described as a tool used for eval...   [0, 0]  \n",
      "1  The weather is described as being quite nice. ...   [0, 0]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@experiment(\n",
    "    name_prefix=\"ragas_faithfullness_test_\",\n",
    "    backend=LocalCSVBackend(root_dir=\".\")\n",
    ")\n",
    "async def run_evaluation(row):   \n",
    "    f_result = await faithfulness_metric.ascore(\n",
    "        user_input=row.user_input,\n",
    "        response=row.response,\n",
    "        retrieved_contexts=row.retrieved_contexts\n",
    "    )\n",
    "    return {\n",
    "        \"user_input\": row.user_input,\n",
    "        \"faithfulness_score\": f_result.value,\n",
    "        \"reasoning\": f_result.reason,\n",
    "        \"statements\": \" | \".join(f_result.traces[\"input\"][\"statements\"]),\n",
    "        \"verdicts\": str(f_result.traces[\"output\"][\"verdicts\"])\n",
    "    }\n",
    "\n",
    "results = await run_evaluation.arun(dataset=dataset)\n",
    "\n",
    "print(\"Evaluation Results\")\n",
    "print(results.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad724b8",
   "metadata": {},
   "source": [
    "**Creation of Statements Prompt**\n",
    "###### To see the ragas prompt for creating statements from the answer, this cell can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85dffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL RAGAS PROMPT\n",
      "Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"description\": \"Structured output for statement generation.\", \"properties\": {\"statements\": {\"description\": \"The generated statements from the answer\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
      "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a German-born theoretical physicist.\",\n",
      "        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n",
      "        \"Albert Einstein was best known for developing the theory of relativity.\",\n",
      "        \"Albert Einstein made important contributions to the development of the theory of quantum mechanics.\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"question\": \"What is Ragas 0.3?\",\n",
      "    \"answer\": \"Ragas 0.3 is a tool to evaluate LLM applications.\"\n",
      "}\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics.collections.faithfulness.util import StatementGeneratorInput\n",
    "\n",
    "sample_input = StatementGeneratorInput(\n",
    "    question=\"What is Ragas 0.3?\", \n",
    "    answer=\"Ragas 0.3 is a tool to evaluate LLM applications.\"\n",
    ")\n",
    "\n",
    "print(\"FULL RAGAS PROMPT\")\n",
    "print(faithfulness_metric.statement_generator_prompt.to_string(sample_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
