{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356244e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contacting Llama 3.2 with the GPU\n",
      "{\n",
      "    \"id\": \"chatcmpl-665\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"As an AI language model, I do not have access to information about specific individuals such as yourself. My knowledge is based on the data and interactions that I have been trained on up until my last update in 2023. Without more context or a specific query, I cannot provide any detailed personal information about you. If you have questions or need assistance with something related to general knowledge or conversation, feel free to ask!\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1768381288,\n",
      "    \"model\": \"qwen2.5:3b\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_ollama\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 87,\n",
      "        \"prompt_tokens\": 36,\n",
      "        \"total_tokens\": 123,\n",
      "        \"completion_tokens_details\": null,\n",
      "        \"prompt_tokens_details\": null\n",
      "    }\n",
      "}\n",
      "Llama says: As an AI language model, I do not have access to information about specific individuals such as yourself. My knowledge is based on the data and interactions that I have been trained on up until my last update in 2023. Without more context or a specific query, I cannot provide any detailed personal information about you. If you have questions or need assistance with something related to general knowledge or conversation, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "## ollama pull llama3.2:3b\n",
    "## ollama run llama3.2:3b\n",
    "## this code is written to see whether ollama is working in  the code or not\n",
    "## It gets 2.7-2.8 GBB of VRAM\n",
    "import json\n",
    "from openai import AsyncOpenAI  # with ollama, the code will mimic the structure of OpenAI API\n",
    "\n",
    "async def test_ollama_response():\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\" # dummy key for placeholder, OpenAI needs api key but our local ollama does not.\n",
    "    )\n",
    "    print(\"Contacting Llama 3.2 with the GPU\")\n",
    "    try:\n",
    "        # await pause this function until the client return an answer, but python can do other tasks on the background\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"qwen2.5:3b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"What do you know about me ?\"}],\n",
    "            timeout=30.0,\n",
    "            n= 3 # we can give parameter to take n number of response if we want to see different results, model run for n times and give responses\n",
    "            #I  tried with n =3, but for ollama compatability, n does not matter, it always return 1.(This feature is  probably working with opeanAI Api )\n",
    "        )\n",
    "        json_data = response.model_dump_json()\n",
    "        parsed_json = json.loads(json_data)\n",
    "        print(json.dumps(parsed_json, indent=4))\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"Llama says: {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Connection Failed: {e}\")\n",
    "\n",
    "await test_ollama_response()\n",
    "# in normal python script, we can use asyncio.run(). But for jupyter, jupyter creates main event loop that manage the events.\n",
    "# we cannot create new event loop with just using asyncio, so we call method with await and main event loop handle it.\n",
    "#Event loops: run asynchronous tasks and callbacks, perform network IO operations, and run subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847168b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
