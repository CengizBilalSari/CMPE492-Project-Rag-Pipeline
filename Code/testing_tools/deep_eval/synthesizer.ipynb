{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8475a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel, OllamaModel\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.synthesizer.config import ContextConstructionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4285b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "            # Simply call the sync version\n",
    "            return self.embed_text(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "            # Simply call the sync version\n",
    "            return self.embed_texts(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return f\"Hugging Face ({self.model_name})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fca46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fc21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_qwen = OllamaModel(model=\"qwen2.5:3b\")\n",
    "hf_embedder = HuggingFaceEmbeddingModel() # Defaults to all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5172f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0613b3c0f8c4d109057fe59e1f9a7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Confident AI Synthesizer Log]</span> SUCCESS: <span style=\"color: #008000; text-decoration-color: #008000\">Successfully deleted</span>: \n",
       "C:\\Users\\CENGIZ~1\\AppData\\Local\\Temp\\deepeval_chroma_2qjc00fc\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Confident AI Synthesizer Log]\u001b[0m SUCCESS: \u001b[32mSuccessfully deleted\u001b[0m: \n",
       "C:\\Users\\CENGIZ~1\\AppData\\Local\\Temp\\deepeval_chroma_2qjc00fc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Confident AI Synthesizer Log]</span> SUCCESS: <span style=\"color: #008000; text-decoration-color: #008000\">Context Construction</span>: Utilizing 8 out of 12 chunks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Confident AI Synthesizer Log]\u001b[0m SUCCESS: \u001b[32mContext Construction\u001b[0m: Utilizing 8 out of 12 chunks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Load (File → LangChain)\\nChunk (Sections → 1024-token strings with overlaps)\\nEmbed (Strings → Vectors)\\nStore (Vectors → ChromaDB)\\nCritique (Random Chunks → LLM Score for whether it can be anchor or not→ Filter)\\nGroup (Seed Chunk → Similarity Search → Final Context ( group the similar chunks and return the contexts))\\nQuestion ( write simple question for the contexts)\\nEvolution (rewrite the simple question for comlexity)\\nshould_style(class StylingConfig:\\n    scenario: Optional[str] = None\\n    task: Optional[str] = None\\n    input_format: Optional[str] = None\\n    expected_output_format: Optional[str] = None\\n)  If user provides this, rewrite the question according to style\\nIncludeExpectedOutput(If we want expected output, give context and question and take the answer from the AI).\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesizer = Synthesizer(model=local_qwen)\n",
    "goldens = synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['../../../Documents/Ragas/ragas_2309.15217v2.pdf'],\n",
    "    max_goldens_per_context=2,\n",
    "    context_construction_config=ContextConstructionConfig(\n",
    "        embedder=hf_embedder ,\n",
    "        critic_model=local_qwen\n",
    "    )\n",
    ")\n",
    "\n",
    "# take documents\n",
    "# load it with  getLoader() \n",
    "\"\"\"self.loader_mapping = {\n",
    "                \".pdf\": lc.PyPDFLoader,\n",
    "                \".txt\": lc.TextLoader,\n",
    "                \".docx\": lc.Docx2txtLoader,\n",
    "                \".md\": lc.TextLoader,\n",
    "                \".markdown\": lc.TextLoader,\n",
    "                \".mdx\": lc.TextLoader,\n",
    "                \"\"\"\n",
    "# document objects are created with loading as sections( e.g. for pdfs, pages are sections)\n",
    "# a_chunk_doc method chunks the objects with TokenTextSplitter\n",
    "\"\"\"Load (File → LangChain)\n",
    "Chunk (Sections → 1024-token strings with overlaps)\n",
    "Embed (Strings → Vectors)\n",
    "Store (Vectors → ChromaDB)\n",
    "Critique (Random Chunks → LLM Score for whether it can be anchor or not→ Filter)\n",
    "Group (Seed Chunk → Similarity Search → Final Context ( group the similar chunks and return the contexts))\n",
    "Question ( write simple question for the contexts)\n",
    "Evolution (rewrite the simple question for comlexity)\n",
    "should_style(class StylingConfig:\n",
    "    scenario: Optional[str] = None\n",
    "    task: Optional[str] = None\n",
    "    input_format: Optional[str] = None\n",
    "    expected_output_format: Optional[str] = None\n",
    ")  If user provides this, rewrite the question according to style\n",
    "IncludeExpectedOutput(If we want expected output, give context and question and take the answer from the AI).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80394005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Golden(input='Faithfulness refers to claims in answers being grounded in context.', actual_output=None, expected_output='Faithfulness is measured by ensuring that the claims made in the generated answer can be inferred from the provided context. This involves extracting statements from the answer and verifying if these statements are supported by the context using a verification function. The final faithfulness score, \\\\( F \\\\), is calculated as the ratio of statements verified to the total number of statements extracted.', context=['we usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023).\\nWe now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2.\\nFaithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer.\\nquestion: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper.\\nin S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format.\\nstatement: [statement 1]\\n...\\nstatement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements.\\nAnswer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer.\\nanswer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion.\\nContext', 'Faith. Ans. Rel. Cont. Rel.\\nRagas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context.\\n5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators).\\nTo put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized.\\nGiven an answer and context, assign a\\nscore for faithfulness in the range 0-10.\\ncontext: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question.\\nIt penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy.\\nquestion: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts.\\n6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance.', ' compare the similarity between\\nthe generated answer and the reference answers.\\nBARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer).\\n3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,'], retrieval_context=None, additional_metadata={'evolutions': ['Multi-context'], 'synthetic_input_quality': 0.7}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None),\n",
       " Golden(input='How does the generated answer address the question of relevance in addressing the role of bees in ecosystems?', actual_output=None, expected_output='To measure how the generated answer addresses the relevance to the question \"How does the generated answer address the role of bees in ecosystems?\", we would use the following approach:\\n\\n1. **Answer Relevance**: We prompt the LLM with a modified version of the original question, asking it to generate potential questions based on the provided answer.\\n2. **Context Relevance**: We then evaluate the relevance of the context used by the system to the generated answer.\\n\\nHere\\'s how we would implement this:\\n\\n### Step 1: Generate Potential Questions\\nGiven the generated answer:\\n```\\nanswer: [Generated Answer]\\n```\\n\\nWe prompt the LLM to generate potential questions based on the provided answer. For example, if the generated answer discusses various aspects of bees\\' roles in ecosystems, the potential questions might include:\\n- What are some key roles bees play in ecosystems?\\n- How do bees contribute to biodiversity?\\n- Can you explain the importance of pollination by bees?\\n\\n### Step 2: Calculate Similarity Scores\\nWe use embeddings from the `text-embedding-ada-002` model to calculate the similarity between these potential questions and the original question:\\n```\\nquestion: [Original Question]\\nanswer: [Generated Answer]\\n\\n# Generate potential questions based on the generated answer\\npotential_questions = generate_potential_questions(answer)\\n\\n# Calculate cosine similarity for each potential question with the original question\\nsimilarity_scores = []\\nfor q in potential_questions:\\n    embeddings_q = text_embedding_ada_002(q)\\n    embeddings_q_original_question = text_embedding_ada_002(question)\\n    sim_score = cosine_similarity(embeddings_q, embeddings_q_original_question)[0][0]\\n    similarity_scores.append(sim_score)\\n\\n# Compute the average similarity score\\naverage_similarity_score = sum(similarity_scores) / len(similarity_scores)\\n```\\n\\n### Step 3: Determine Answer Relevance Score (AR)\\nWe compute the answer relevance score using the formula provided:\\n```\\nanswer_relevance_score = 1 / n * sum(similarity_scores)\\n```\\n\\nWhere `n` is the number of potential questions generated.\\n\\n### Final Answer Relevance Score\\nThe final answer relevance score, AR, for addressing the question \"How does the generated answer address the role of bees in ecosystems?\" would be:\\n```\\nAR = average_similarity_score\\n```\\n\\nThis score indicates how closely the generated answer aligns with the original question or instruction regarding its relevance to the topic.', context=['we usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023).\\nWe now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2.\\nFaithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer.\\nquestion: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper.\\nin S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format.\\nstatement: [statement 1]\\n...\\nstatement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements.\\nAnswer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer.\\nanswer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion.\\nContext', 'Faith. Ans. Rel. Cont. Rel.\\nRagas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context.\\n5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators).\\nTo put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized.\\nGiven an answer and context, assign a\\nscore for faithfulness in the range 0-10.\\ncontext: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question.\\nIt penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy.\\nquestion: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts.\\n6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance.', ' compare the similarity between\\nthe generated answer and the reference answers.\\nBARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer).\\n3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,'], retrieval_context=None, additional_metadata={'evolutions': ['Concretizing'], 'synthetic_input_quality': 0.7}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None),\n",
       " Golden(input='Compare Christopher Nolan’s directed Oppenheimer and Cillian Murphy’s role in the film.', actual_output=None, expected_output='Christopher Nolan directed the film \"Oppenheimer,\" a biographical thriller based on J. Robert Oppenheimer\\'s life, including his pivotal role in developing nuclear weapons during World War II. In this film, Cillian Murphy portrayed J. Robert Oppenheimer.', context=['Question Context Answer\\nWho directed the film Op-\\npenheimer and who stars\\nas J. Robert Oppenheimer\\nin the film?\\nOppenheimer is a 2023 biographical thriller film written\\nand directed by Christopher Nolan. Based on the 2005\\nbiography American Prometheus by Kai Bird and Mar-\\ntin J. Sherwin, the film chronicles the life of J. Robert\\nOppenheimer, a theoretical physicist who was pivotal in\\ndeveloping the first nuclear weapons as part of the Man-\\nhattan Project, and thereby ushering in the Atomic Age.\\nCillian Murphy stars as Oppenheimer, with Emily Blunt\\nas Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\\nHigh Faithfulness : Christopher\\nNolan directed the film Oppen-\\nheimer. Cillian Murphy stars as J.\\nRobert Oppenheimer in the film.\\nLow Faithfulness : James\\nCameron directed the film Op-\\npenheimer. Tom Cruise stars as J.\\nRobert Oppenheimer in the film.\\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\\nQuestion Answer\\nWhen is the scheduled\\nlaunch date and time for\\nthe PSLV-C56 mission,\\nand where will it be\\nlaunched from?\\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\\nCentre, Sriharikota, Andhra Pradesh, India.\\nLow answer relevance: The scheduled launch date and time for the PSLV-C56 mission have\\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\\nlaunch a satellite into orbit to study weather patterns.\\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\\nQuestion Context\\nWhen was the Chimnabai\\nClock Tower completed,\\nand who was it named af-\\nter?\\nHigh context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State.\\nLow context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\\nHistory. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\\nI (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\\ninaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\\nof 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\\nTable 4: Example from WikiEval, showing answers with high and low context relevance.', ' the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas.\\narXiv:2309.15217v2  [cs.CL]  28 Apr 2025'], retrieval_context=None, additional_metadata={'evolutions': ['Comparative'], 'synthetic_input_quality': 1.0}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None),\n",
       " Golden(input='When did the Sayajibai Clock Tower complete and name by Sayajirao Gaekwad III?', actual_output=None, expected_output='The Sayajibai Clock Tower, also known as the Raopura Clock Tower, was completed in 1896 and named after Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda.', context=['Question Context Answer\\nWho directed the film Op-\\npenheimer and who stars\\nas J. Robert Oppenheimer\\nin the film?\\nOppenheimer is a 2023 biographical thriller film written\\nand directed by Christopher Nolan. Based on the 2005\\nbiography American Prometheus by Kai Bird and Mar-\\ntin J. Sherwin, the film chronicles the life of J. Robert\\nOppenheimer, a theoretical physicist who was pivotal in\\ndeveloping the first nuclear weapons as part of the Man-\\nhattan Project, and thereby ushering in the Atomic Age.\\nCillian Murphy stars as Oppenheimer, with Emily Blunt\\nas Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\\nHigh Faithfulness : Christopher\\nNolan directed the film Oppen-\\nheimer. Cillian Murphy stars as J.\\nRobert Oppenheimer in the film.\\nLow Faithfulness : James\\nCameron directed the film Op-\\npenheimer. Tom Cruise stars as J.\\nRobert Oppenheimer in the film.\\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\\nQuestion Answer\\nWhen is the scheduled\\nlaunch date and time for\\nthe PSLV-C56 mission,\\nand where will it be\\nlaunched from?\\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\\nCentre, Sriharikota, Andhra Pradesh, India.\\nLow answer relevance: The scheduled launch date and time for the PSLV-C56 mission have\\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\\nlaunch a satellite into orbit to study weather patterns.\\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\\nQuestion Context\\nWhen was the Chimnabai\\nClock Tower completed,\\nand who was it named af-\\nter?\\nHigh context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State.\\nLow context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\\nHistory. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\\nI (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\\ninaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\\nof 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\\nTable 4: Example from WikiEval, showing answers with high and low context relevance.', ' the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas.\\narXiv:2309.15217v2  [cs.CL]  28 Apr 2025'], retrieval_context=None, additional_metadata={'evolutions': ['Comparative'], 'synthetic_input_quality': 0.2}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None),\n",
       " Golden(input='How does Demonstrate-search-predict by Matei Zaharia contribute to knowledge-intensive NLP tasks?', actual_output=None, expected_output='Matei Zaharia\\'s \"Demonstrate-search-predict\" contributes to knowledge-intensive Natural Language Processing (NLP) tasks by proposing a method for composing retrieval and language models. This approach enables LLMs to use knowledge from a reference textual database, thereby acting as a natural language layer between users and databases, which can reduce the risk of hallucinations. The paper introduces \"Demonstrate-search-predict\" as a framework for evaluating Retrieval-Augmented Generation (RAG) pipelines without relying on ground truth human annotations.', context=[' and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do-\\nmain question answering. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics, pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\\nscale hallucination evaluation benchmark for large\\nlanguage models. CoRR, abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\\njape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. 2023. Lost in the middle: How language\\nmodels use long contexts.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 9802–9822, Toronto,\\nCanada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels. CoRR, abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.', 'Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs.\\n1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs.\\nWhile the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c).\\nMoreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used.\\nTo address these issues, in this paper we present\\nRagas1, a framework for', 'Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\\nang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\\nZhou. 2023a. Is chatgpt a good NLG evaluator? A\\npreliminary study. CoRR, abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b. Large language models are not fair evaluators.\\nCoRR, abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna\\nGarimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c. KNN-LM does not improve open-ended text\\ngeneration. CoRR, abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera-\\ntion. In Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Informa-\\ntion Processing Systems 2021, NeurIPS 2021, De-\\ncember 6-14, 2021, virtual, pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\\nDanny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking. CoRR,\\nabs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\\nating text generation with BERT. In8th International\\nConference on Learning Representations, ICLR 2020,\\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nA Examples from WikiEval\\nTables 2, 3 and 4 show examples from the WikiEval\\ndataset, focusing in particular on answers with high\\nand low faithfulness (Table 2), high and low answer\\nrelevance (Table 3), and high and low context rele-\\nvance (Table 4).'], retrieval_context=None, additional_metadata={'evolutions': ['Concretizing'], 'synthetic_input_quality': 0.3}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None),\n",
       " Golden(input=\"What aspects does Matei Zaharia's paper 'Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP' discuss?\", actual_output=None, expected_output='Matei Zaharia\\'s paper \"Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP\" discusses the composition of retrieval and language models for knowledge-intensive Natural Language Processing (NLP). Specifically, it focuses on how these models can be used to compose a system that leverages both retrieval capabilities from a textual database and generation modules based on Large Language Models (LLMs) to provide LLMs with relevant context. The paper does not delve into specific aspects or metrics for evaluating such systems.', context=[' and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do-\\nmain question answering. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics, pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\\nscale hallucination evaluation benchmark for large\\nlanguage models. CoRR, abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\\njape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. 2023. Lost in the middle: How language\\nmodels use long contexts.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 9802–9822, Toronto,\\nCanada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels. CoRR, abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.', 'Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs.\\n1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs.\\nWhile the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c).\\nMoreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used.\\nTo address these issues, in this paper we present\\nRagas1, a framework for', 'Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\\nang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\\nZhou. 2023a. Is chatgpt a good NLG evaluator? A\\npreliminary study. CoRR, abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b. Large language models are not fair evaluators.\\nCoRR, abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna\\nGarimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c. KNN-LM does not improve open-ended text\\ngeneration. CoRR, abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera-\\ntion. In Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Informa-\\ntion Processing Systems 2021, NeurIPS 2021, De-\\ncember 6-14, 2021, virtual, pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\\nDanny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking. CoRR,\\nabs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\\nating text generation with BERT. In8th International\\nConference on Learning Representations, ICLR 2020,\\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nA Examples from WikiEval\\nTables 2, 3 and 4 show examples from the WikiEval\\ndataset, focusing in particular on answers with high\\nand low faithfulness (Table 2), high and low answer\\nrelevance (Table 3), and high and low context rele-\\nvance (Table 4).'], retrieval_context=None, additional_metadata={'evolutions': ['Reasoning'], 'synthetic_input_quality': 0.6}, comments=None, tools_called=None, expected_tools=None, source_file='../../../Documents/Ragas/ragas_2309.15217v2.pdf', name=None, custom_column_key_values=None, multimodal=False, images_mapping=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40037fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>n_chunks_per_context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>evolutions</th>\n",
       "      <th>context_quality</th>\n",
       "      <th>synthetic_input_quality</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faithfulness refers to claims in answers being...</td>\n",
       "      <td>None</td>\n",
       "      <td>Faithfulness is measured by ensuring that the ...</td>\n",
       "      <td>[we usually do not have access to human-annota...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>8710</td>\n",
       "      <td>[Multi-context]</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the generated answer address the ques...</td>\n",
       "      <td>None</td>\n",
       "      <td>To measure how the generated answer addresses ...</td>\n",
       "      <td>[we usually do not have access to human-annota...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>8710</td>\n",
       "      <td>[Concretizing]</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compare Christopher Nolan’s directed Oppenheim...</td>\n",
       "      <td>None</td>\n",
       "      <td>Christopher Nolan directed the film \"Oppenheim...</td>\n",
       "      <td>[Question Context Answer\\nWho directed the fil...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2974</td>\n",
       "      <td>[Comparative]</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When did the Sayajibai Clock Tower complete an...</td>\n",
       "      <td>None</td>\n",
       "      <td>The Sayajibai Clock Tower, also known as the R...</td>\n",
       "      <td>[Question Context Answer\\nWho directed the fil...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2974</td>\n",
       "      <td>[Comparative]</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does Demonstrate-search-predict by Matei Z...</td>\n",
       "      <td>None</td>\n",
       "      <td>Matei Zaharia's \"Demonstrate-search-predict\" c...</td>\n",
       "      <td>[ and\\nMatei Zaharia. 2022. Demonstrate-search...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>8796</td>\n",
       "      <td>[Concretizing]</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What aspects does Matei Zaharia's paper 'Demon...</td>\n",
       "      <td>None</td>\n",
       "      <td>Matei Zaharia's paper \"Demonstrate-search-pred...</td>\n",
       "      <td>[ and\\nMatei Zaharia. 2022. Demonstrate-search...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>8796</td>\n",
       "      <td>[Reasoning]</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6</td>\n",
       "      <td>../../../Documents/Ragas/ragas_2309.15217v2.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input actual_output  \\\n",
       "0  Faithfulness refers to claims in answers being...          None   \n",
       "1  How does the generated answer address the ques...          None   \n",
       "2  Compare Christopher Nolan’s directed Oppenheim...          None   \n",
       "3  When did the Sayajibai Clock Tower complete an...          None   \n",
       "4  How does Demonstrate-search-predict by Matei Z...          None   \n",
       "5  What aspects does Matei Zaharia's paper 'Demon...          None   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  Faithfulness is measured by ensuring that the ...   \n",
       "1  To measure how the generated answer addresses ...   \n",
       "2  Christopher Nolan directed the film \"Oppenheim...   \n",
       "3  The Sayajibai Clock Tower, also known as the R...   \n",
       "4  Matei Zaharia's \"Demonstrate-search-predict\" c...   \n",
       "5  Matei Zaharia's paper \"Demonstrate-search-pred...   \n",
       "\n",
       "                                             context retrieval_context  \\\n",
       "0  [we usually do not have access to human-annota...              None   \n",
       "1  [we usually do not have access to human-annota...              None   \n",
       "2  [Question Context Answer\\nWho directed the fil...              None   \n",
       "3  [Question Context Answer\\nWho directed the fil...              None   \n",
       "4  [ and\\nMatei Zaharia. 2022. Demonstrate-search...              None   \n",
       "5  [ and\\nMatei Zaharia. 2022. Demonstrate-search...              None   \n",
       "\n",
       "   n_chunks_per_context  context_length       evolutions context_quality  \\\n",
       "0                     3            8710  [Multi-context]            None   \n",
       "1                     3            8710   [Concretizing]            None   \n",
       "2                     2            2974    [Comparative]            None   \n",
       "3                     2            2974    [Comparative]            None   \n",
       "4                     3            8796   [Concretizing]            None   \n",
       "5                     3            8796      [Reasoning]            None   \n",
       "\n",
       "   synthetic_input_quality                                      source_file  \n",
       "0                      0.7  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  \n",
       "1                      0.7  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  \n",
       "2                      1.0  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  \n",
       "3                      0.2  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  \n",
       "4                      0.3  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  \n",
       "5                      0.6  ../../../Documents/Ragas/ragas_2309.15217v2.pdf  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = synthesizer.to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233f087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
