{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356244e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contacting Llama 3.2 with the GPU\n",
      "{\n",
      "    \"id\": \"chatcmpl-58\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"I don't know anything about you personally. I'm a large language model, I don't have the ability to gather or store any information about individuals. Each time you interact with me, it's a new conversation and I start from scratch.\\n\\nHowever, based on our conversation, I can infer that:\\n\\n1. You're using a device connected to the internet, which allows you to access my services.\\n2. You're able to type and communicate with me in English (or another language, if you prefer).\\n3. You're curious about me and want to know more about how I work or what I can do.\\n\\nThat's all I can infer from our conversation so far. If you'd like to share more about yourself, I'm here to listen!\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1768286066,\n",
      "    \"model\": \"llama3.2:3b\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_ollama\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 154,\n",
      "        \"prompt_tokens\": 32,\n",
      "        \"total_tokens\": 186,\n",
      "        \"completion_tokens_details\": null,\n",
      "        \"prompt_tokens_details\": null\n",
      "    }\n",
      "}\n",
      "Llama says: I don't know anything about you personally. I'm a large language model, I don't have the ability to gather or store any information about individuals. Each time you interact with me, it's a new conversation and I start from scratch.\n",
      "\n",
      "However, based on our conversation, I can infer that:\n",
      "\n",
      "1. You're using a device connected to the internet, which allows you to access my services.\n",
      "2. You're able to type and communicate with me in English (or another language, if you prefer).\n",
      "3. You're curious about me and want to know more about how I work or what I can do.\n",
      "\n",
      "That's all I can infer from our conversation so far. If you'd like to share more about yourself, I'm here to listen!\n"
     ]
    }
   ],
   "source": [
    "## ollama pull llama3.2:3b\n",
    "## ollama run llama3.2:3b\n",
    "## this code is written to see whether ollama is working in  the code or not\n",
    "## It gets 2.7-2.8 GBB of VRAM\n",
    "import json\n",
    "from openai import AsyncOpenAI  # with ollama, the code will mimic the structure of OpenAI API\n",
    "\n",
    "async def test_ollama_response():\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\" # dummy key for placeholder, OpenAI needs api key but our local ollama does not.\n",
    "    )\n",
    "    print(\"Contacting Llama 3.2 with the GPU\")\n",
    "    try:\n",
    "        # await pause this function until the client return an answer, but python can do other tasks on the background\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"llama3.2:3b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"What do you know about me ?\"}],\n",
    "            timeout=30.0,\n",
    "            n= 3 # we can give parameter to take n number of response if we want to see different results, model run for n times and give responses\n",
    "            #I  tried with n =3, but for ollama compatability, n does not matter, it always return 1.(This feature is  probably working with opeanAI Api )\n",
    "        )\n",
    "        json_data = response.model_dump_json()\n",
    "        parsed_json = json.loads(json_data)\n",
    "        print(json.dumps(parsed_json, indent=4))\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"Llama says: {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Connection Failed: {e}\")\n",
    "\n",
    "await test_ollama_response()\n",
    "# in normal python script, we can use asyncio.run(). But for jupyter, jupyter creates main event loop that manage the events.\n",
    "# we cannot create new event loop with just using asyncio, so we call method with await and main event loop handle it.\n",
    "#Event loops: run asynchronous tasks and callbacks, perform network IO operations, and run subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847168b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
