# GraphRAG: A Deep Technical Overview

> **Graph-based Retrieval Augmented Generation** - Microsoft Research's approach to unlocking LLM discovery on narrative private data.

[![Paper](https://img.shields.io/badge/arXiv-2404.16130-red)](https://arxiv.org/pdf/2404.16130)
[![Blog](https://img.shields.io/badge/Microsoft-Research%20Blog-blue)](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)

---

## ðŸ“‹ Table of Contents

- [Introduction](#introduction)
- [Why GraphRAG?](#why-graphrag)
- [System Architecture](#system-architecture)
- [Phase 1: Indexing Pipeline](#phase-1-indexing-pipeline)
- [Phase 2: Query Engine](#phase-2-query-engine)
- [Global Search Deep Dive](#global-search-deep-dive)
- [Key Design Decisions](#key-design-decisions)

---

## Introduction

GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG) that uses **LLM-generated knowledge graphs** to significantly improve question-answering capabilities on private datasets. Unlike traditional vector-based RAG systems, GraphRAG can:

- Connect disparate pieces of information across documents
- Answer holistic questions about entire datasets
- Provide provenance and source grounding for generated answers
- Support **self-reflection** where the LLM validates its own responses

The data is organized **hierarchically**, enabling both:
- **General to Specific**: Top-down exploration from themes to details
- **Specific to General**: Bottom-up aggregation from entities to patterns

---

## Why GraphRAG?

Traditional Baseline RAG fails in two critical scenarios:

| Scenario | Example Query | Why Baseline RAG Fails |
|----------|--------------|----------------------|
| **Connecting the Dots** | "What has Novorossiya done?" | Vector search can't traverse relationships between entities |
| **Holistic Understanding** | "What are the main themes in this dataset?" | No single text chunk contains dataset-wide themes |

GraphRAG solves these by building a **knowledge graph** with **hierarchical community structure**, enabling both local entity reasoning and global dataset understanding.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            GraphRAG SYSTEM                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                     PHASE 1: INDEXING                               â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚   Documents â†’ Chunks â†’ Entities/Relations â†’ Graph â†’ Communities     â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                     PHASE 2: QUERY                                  â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚   â”‚   â”‚  Global  â”‚ â”‚  Local   â”‚ â”‚  DRIFT   â”‚ â”‚  Basic   â”‚               â”‚   â”‚
â”‚   â”‚   â”‚  Search  â”‚ â”‚  Search  â”‚ â”‚  Search  â”‚ â”‚  Search  â”‚               â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 1: Indexing Pipeline

The indexing pipeline transforms raw documents into a queryable knowledge structure through 5 stages:

### Stage A: Text Chunking

Documents are split into **TextUnits** - analyzable chunks that serve as atomic units for extraction.

```
Document (50,000 tokens)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1   â”‚  Chunk 2   â”‚  ...  â”‚  Chunk N  â”‚
â”‚  600 tok   â”‚  600 tok   â”‚       â”‚  600 tok  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Default chunk size**: ~600 tokens (configurable)
- Smaller chunks = higher fidelity extraction
- Larger chunks = faster processing, lower cost

### Stage B: Entity, Relationship & Claims Extraction

The LLM analyzes each chunk to extract structured information:

| Extraction Type | Description | Example |
|----------------|-------------|---------|
| **Entities** | People, places, organizations, events | "Donald Trump", "New York", "Tesla" |
| **Relationships** | Connections between entities | Trump â†’ FOUNDED â†’ Trump Organization |
| **Claims** | Time-bound factual statements | "Trump became CEO of Trump Org in 1971" |

Claims capture the **who, what, when, where** with temporal bounds and evaluated status.

### Stage C: Knowledge Graph Construction

Individual subgraphs from each chunk are merged into a unified knowledge graph:

```
Chunk 1: [Trump] â”€â”€works_atâ”€â”€â–¶ [Trump Org]
Chunk 2: [Trump] â”€â”€foundedâ”€â”€â–¶ [Trump Org]
Chunk 3: [Trump] â”€â”€lives_inâ”€â”€â–¶ [NYC]
                     â”‚
                     â–¼ MERGE
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Unified Knowledge      â”‚
         â”‚         Graph             â”‚
         â”‚                           â”‚
         â”‚  [Trump]â”€â”€â”¬â”€â”€works_atâ”€â”€â”  â”‚
         â”‚           â”œâ”€â”€foundedâ”€â”€â”€â”¼â”€â–¶[Trump Org]
         â”‚           â””â”€â”€lives_inâ”€â”€â”˜  â”‚
         â”‚                  â”‚        â”‚
         â”‚                  â–¼        â”‚
         â”‚               [NYC]       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key merging rules:**
- Entities with same name/type â†’ merged into single node
- Multiple descriptions â†’ summarized by LLM
- Edge weights â†’ based on relationship **frequency** across chunks

### Stage D: Community Detection

The **Leiden algorithm** performs hierarchical clustering on the graph:

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Level 0        â”‚  â—€â”€â”€ Entire graph (1 community)
                     â”‚   "All Entities"   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚  Level 1  â”‚    â”‚  Level 1  â”‚    â”‚  Level 1  â”‚
        â”‚ "Politics"â”‚    â”‚ "Business"â”‚    â”‚ "Sports"  â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â”‚           â”‚    â”‚           â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Level 2  â”‚ â”‚Level 2  â”‚ â”‚Level 2  â”‚
   â”‚"US Pol" â”‚ â”‚"EU Pol" â”‚ â”‚"Finance"â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This hierarchical structure enables queries at different **granularity levels**.

### Stage E: Community Summarization

Each community receives an LLM-generated summary report:

```markdown
## Community: US Political Entities

### Executive Summary
This community encompasses key political figures and organizations
involved in US domestic policy...

### Key Entities
- Donald Trump (Person) - 45th President
- Joe Biden (Person) - 46th President  
- White House (Place) - Executive residence

### Key Findings
1. Strong relationship network between political figures and lobbyists
2. Recurring themes of policy disputes on economic matters
3. [Data: Reports (12, 45, 67)]
```

**Bottom-up summarization**: Lower-level community summaries are passed up to inform higher-level summaries, creating coherent hierarchical understanding.

---

## Phase 2: Query Engine

GraphRAG supports 4 distinct query modes:

### 1. Global Search ðŸŒ

**Purpose**: Answer holistic questions about the entire dataset

| Question Type | Example |
|--------------|---------|
| Thematic | "What are the main themes in this dataset?" |
| Aggregate | "What are the most common conflict patterns?" |
| Summary | "Give me an overview of all political events" |

Uses **Map-Reduce** pattern with community summaries. [See detailed explanation below](#global-search-deep-dive).

### 2. Local Search ðŸ”

**Purpose**: Detailed questions about specific entities

| Question Type | Example |
|--------------|---------|
| Entity-specific | "When and where did Donald Trump enter business?" |
| Relationship | "What is the connection between Company X and Person Y?" |
| Timeline | "What events involved Entity Z in 2023?" |

**Mechanism**: Starts from query entities, fans out through graph relationships to gather relevant context.

### 3. DRIFT Search ðŸš€

**Purpose**: Enhanced local search with community context

DRIFT (Dynamic Reasoning and Inference with Flexible Traversal) adds community summaries to the context window, providing:
- Broader thematic context for entity-focused queries
- Better understanding of entity's role within communities
- Hybrid local + global perspective

### 4. Basic Search ðŸ“Š

**Purpose**: Standard vector similarity search (Baseline RAG)

- Traditional top-k retrieval based on embedding similarity
- Useful when query is well-represented in individual text chunks
- Falls back to this for simple factual lookups

---

## Global Search Deep Dive

Global Search is the most sophisticated query mode, using a **Map-Reduce** approach to synthesize information across all communities.

### The Challenge

```
Problem: 500 community reports Ã— 500 tokens = 250,000 tokens
         LLM context window = ~128,000 tokens
         
         âŒ Cannot fit all reports in one prompt!
```

### Solution: Map-Reduce with Batching

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GLOBAL SEARCH FLOW                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Step 1: CONTEXT BUILDING                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚     500 Community Reports                       â”‚                   â”‚
â”‚   â”‚  [R1][R2][R3]...[R100]...[R200]...[R500]        â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                           â”‚                                             â”‚
â”‚                           â–¼ random.shuffle()                            â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚  [R234][R12][R456][R89]...[R7][R301]...[R99]    â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                           â”‚                                             â”‚
â”‚                           â–¼ Split by token limit (8000 tokens/batch)    â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚ Batch 1  â”‚  â”‚ Batch 2  â”‚  â”‚ Batch 3  â”‚  â”‚ Batch N  â”‚                â”‚
â”‚   â”‚ ~50 reps â”‚  â”‚ ~50 reps â”‚  â”‚ ~50 reps â”‚  â”‚ ~50 reps â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚
â”‚        â”‚             â”‚             â”‚             â”‚                      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                             â”‚                                           â”‚
â”‚   Step 2: MAP (Parallel)    â–¼                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  For each batch (in parallel via asyncio.gather):                â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  Prompt: "Given these reports, answer the user's question.       â”‚  â”‚
â”‚   â”‚           Return key points with importance scores (0-100)."     â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  Response format:                                                â”‚  â”‚
â”‚   â”‚  {                                                               â”‚  â”‚
â”‚   â”‚    "points": [                                                   â”‚  â”‚
â”‚   â”‚      {"description": "Theme 1... [Data: Reports (2,7)]", "score": 85},  â”‚
â”‚   â”‚      {"description": "Theme 2... [Data: Reports (12)]", "score": 72}â”‚
â”‚   â”‚    ]                                                             â”‚  â”‚
â”‚   â”‚  }                                                               â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                                           â”‚
â”‚                             â–¼                                           â”‚
â”‚   Step 3: FILTER & RANK                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  1. Collect all points from all batches                          â”‚  â”‚
â”‚   â”‚  2. Filter out points with score = 0                             â”‚  â”‚
â”‚   â”‚  3. Sort by score DESCENDING                                     â”‚  â”‚
â”‚   â”‚  4. Take top points until max_data_tokens reached                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                                           â”‚
â”‚                             â–¼                                           â”‚
â”‚   Step 4: REDUCE                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Prompt: "Synthesize these analyst reports into a final answer"  â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  ----Analyst 1----                                               â”‚  â”‚
â”‚   â”‚  Importance Score: 95                                            â”‚  â”‚
â”‚   â”‚  The primary theme is conflict resolution [Data: Reports (1,5)]  â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  ----Analyst 2----                                               â”‚  â”‚
â”‚   â”‚  Importance Score: 88                                            â”‚  â”‚
â”‚   â”‚  Economic factors play a key role [Data: Reports (3,7)]          â”‚  â”‚
â”‚   â”‚  ...                                                             â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  â†’ LLM generates final comprehensive answer                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                                           â”‚
â”‚                             â–¼                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  FINAL ANSWER with source citations [Data: Reports (...)]       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Random Shuffle?

The shuffle before batching is a **critical design decision**:

| Without Shuffle | With Shuffle |
|----------------|--------------|
| Batch 1: All politics reports | Batch 1: Mixed topics |
| Batch 2: All economics reports | Batch 2: Mixed topics |
| Each batch has narrow perspective | Each batch has diverse perspective |
| Single topic failure = total loss | Redundancy across batches |
| Biased toward early communities | Fair representation |

**Code reference** (`community_context.py`):
```python
if shuffle_data:
    random.seed(random_state)  # random_state=86 for reproducibility
    random.shuffle(selected_reports)
```

### Importance Scoring

Each point from MAP phase includes a score (0-100):

- **0**: No relevant information ("I don't know")
- **1-30**: Tangentially related
- **31-70**: Moderately relevant
- **71-100**: Highly relevant, directly answers query

Only points with `score > 0` proceed to the REDUCE phase.

---

## Key Design Decisions

### 1. Hierarchical Data Organization

```
Level 0 (Coarse) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Level N (Fine)
     â”‚                                                    â”‚
     â”‚  "What are the main themes?"                       â”‚  "What did person X do on date Y?"
     â”‚                                                    â”‚
     â””â”€â”€â”€â”€ Use high-level community summaries             â””â”€â”€â”€â”€ Use detailed entity relationships
```

### 2. Self-Reflection / Validation

The LLM performs internal validation:
- Importance scoring acts as self-assessment
- Points with score=0 are filtered (LLM admits uncertainty)
- Source citations enable verification

### 3. Provenance & Grounding

Every claim includes source references:
```
"Person X is involved in controversy [Data: Reports (2, 7, 34)]"
```
This enables:
- Human verification of claims
- Traceability to original documents
- Reduced hallucination risk

### 4. Token Budget Management

```python
max_context_tokens = 8000  # Per batch in MAP phase
max_data_tokens = 8000     # For aggregated points in REDUCE phase
```

Careful token management ensures:
- Consistent batch sizes
- Predictable API costs
- Reliable response generation

---

## References

- **Paper**: [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)
- **Blog**: [GraphRAG: Unlocking LLM discovery on narrative private data](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)
- **Documentation**: [Microsoft GraphRAG Docs](https://microsoft.github.io/graphrag)

---

*This document provides a technical deep-dive into GraphRAG. For getting started, see the official [Quick Start Guide](https://microsoft.github.io/graphrag/get_started/).*
