{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc8f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d6c520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Patient\", \"shows\", \"signs of Bradycardia\"],\n",
      "       [\"Bradycardia\", \"is associated with\", \"HCN4 gene mutation\"]\n",
      "   ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_triplets(text):\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert Knowledge Graph engineer. Your task is to extract relationships from text in the form of (subject, relation, object) triplets.\n",
    "    \n",
    "    GUIDELINES:\n",
    "    1. Subjects and Objects should be concise nouns or entities (e.g., \"Ragas\", \"LLM\", \"Evaluation Framework\").\n",
    "    2. Relations should be verbs or verb phrases representing the link (e.g., \"is designed for\", \"measures\", \"improves\").\n",
    "    3. If a sentence is complex, break it into multiple simple triplets.\n",
    "    4. Ensure the output is strictly valid JSON.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    Text: \"Ragas uses LLMs to automate the evaluation of RAG pipelines.\"\n",
    "    Output: {\n",
    "      \"triplets\": [\n",
    "        [\"Ragas\", \"uses\", \"LLMs\"],\n",
    "        [\"Ragas\", \"automates\", \"evaluation of RAG pipelines\"]\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\":system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Extract entities from this text: {text}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0, # it should be  deterministic for KG extraction\n",
    "        response_format={\"type\": \"json_object\"} # to get valid JSON back\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Example \n",
    "sample_text = \"Patient shows signs of Bradycardia which is often associated with the HCN4 gene mutation.\"\n",
    "print(extract_triplets(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e58be722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c011414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propositions(text):\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": (\n",
    "                    \"Decompose the input text into standalone, atomic facts. \"\n",
    "                    \"Return the result in JSON format with a single key called 'propositions' \"\n",
    "                    \"which contains a list of strings. \"\n",
    "                    \"Example: {'propositions': ['fact 1', 'fact 2']}\" \n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Decompose this text: {text}\"\n",
    "            }\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    content = json.loads(response.choices[0].message.content)\n",
    "    return content.get(\"propositions\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fada199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_grouping(propositions):\n",
    "    # SemanticChunker expects a single string or list of docs\n",
    "    chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "    full_text = \" \".join(propositions)\n",
    "    # This splits the text at points where the \"meaning\" shifts\n",
    "    chunks = chunker.create_documents([full_text])\n",
    "    return [chunk.page_content for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e20ec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_neo4j(triplets):\n",
    "    uri = os.getenv(\"NEO4J_URI\")\n",
    "    user = os.getenv(\"NEO4J_USERNAME\") \n",
    "    password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for triplet in triplets:\n",
    "            # it's a valid 3-item list or not\n",
    "            if isinstance(triplet, list) and len(triplet) == 3:\n",
    "                s, r, o = triplet\n",
    "                \n",
    "                # Convention\n",
    "                rel_type = str(r).replace(\" \", \"_\").upper()\n",
    "                \n",
    "                session.run(\"\"\"\n",
    "                    MERGE (a:Entity {name: $s})\n",
    "                    MERGE (b:Entity {name: $o})\n",
    "                    WITH a, b\n",
    "                    CALL apoc.merge.relationship(a, $rel_type, {}, {}, b) YIELD rel\n",
    "                    RETURN rel\n",
    "                \"\"\", s=str(s), rel_type=rel_type, o=str(o))\n",
    "            else:\n",
    "                print(f\"Skipping malformed triplet: {triplet}\")\n",
    "                \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "865732a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "def load_local_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n",
    "\n",
    "    ext = os.path.splitext(file_path)[-1].lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    elif ext == \".docx\":\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "    docs = loader.load()\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3ab1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "structural_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df7cff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Documents\\Ragas\\ragas_2309.15217v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Facts: 100%|██████████| 22/22 [01:56<00:00,  5.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "target_file = r\"C:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Documents\\Ragas\\ragas_2309.15217v2.pdf\"\n",
    "print(f\"Loading file: {target_file}\")\n",
    "raw_content = load_local_file(target_file)\n",
    "rough_chunks = structural_splitter.split_text(raw_content)\n",
    "\n",
    "all_propositions = []\n",
    "\n",
    "for segment in tqdm(rough_chunks, desc=\"Extracting Facts\"):\n",
    "    props = get_propositions(segment) \n",
    "    all_propositions.extend(props)\n",
    "    \n",
    "semantic_chunks = semantic_grouping(all_propositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a1264d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ragas is a framework for reference-free evaluation of Retrieval Augmented Generation pipelines. RAG systems are composed of a retrieval and an LLM based generation module. RAG systems provide LLMs with knowledge from a reference textual database. RAG systems reduce the risk of hallucinations by acting as a natural language layer between a user and textual databases. Evaluating RAG architectures is challenging because of several dimensions to consider. The dimensions to consider in RAG architectures include the ability to identify relevant and focused context passages. The dimensions to consider in RAG architectures include the ability to exploit passages in a faithful way. The dimensions to consider in RAG architectures include the quality of the generation itself. Ragas provides a suite of metrics to evaluate different dimensions without relying on ground truth human annotations. A framework like Ragas can contribute to faster evaluation cycles of RAG architectures. RAG architectures are adopted quickly due to the fast adoption of LLMs. Language Models capture a vast amount of knowledge about the world. Language Models can answer questions without accessing any external sources. The idea of LMs as repositories of knowledge emerged after the introduction of BERT. The introduction of ever larger LMs firmly established the idea of LMs as repositories of knowledge. Recent Large Language Models capture enough knowledge to rival human performance across various question answering benchmarks. Large Language Models achieve human performance across a wide variety of question answering benchmarks. The fast adoption of LLMs is driven by a need for faster evaluation cycles of RAG architectures. Large Language Models (LLMs) have enough knowledge to rival human performance. LLMs are not able to answer questions about events that have happened after they were trained. Even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus. Retrieval Augmented Generation (RAG) is the standard solution to the issues faced by LLMs. Answering a question essentially involves retrieving relevant passages from a corpus and feeding these passages, along with the original question, to the LM. Simply adding retrieved documents to the input of a standard LM can work well. The implementation of retrieval-augmented strategies requires a significant amount of tuning. The performance of retrieval-augmented systems will be affected by the retrieval model, the considered corpus, the LM, or the prompt formulation, among others. Automated evaluation of retrieval-augmented systems is paramount. RAG systems are often evaluated in terms of the language modelling task itself. Evaluating retrieval-augmented systems in terms of perplexity on some reference corpus is not always predictive of downstream performance. The evaluation strategy that relies on LM probabilities is not accessible for some language models. evaluations are not always predictive of downstream performance evaluation strategy relies on LM probabilities LM probabilities are not accessible for some closed models ChatGPT and GPT-4 are closed models question answering is a common evaluation task datasets with short extractive answers are not representative of how the system will be used the paper presents Ragas, a framework for automated assessment Ragas is a framework for automated assessment is available at https://github.com/explodinggradients/ragas the paper has arXiv:2309.15217v2 [cs.CL] 28 Apr 2025 ID this is not an evaluation issue with Wang et al. (2023c) Retrieval augmented generation systems exist. Reference answers may not be available in some settings.',\n",
       " 'Different proxies for correctness may be needed in addition to the usefulness of retrieved passages. The Ragas framework integrates with llama-index and Langchain. Llama-index and Langchain are widely used frameworks for building RAG solutions. Ragas can be integrated into standard workflows easily. Estimating faithfulness using LLMs is a studied problem. Detecting hallucinations in LLM generated responses has been extensively studied. Predicting factuality using a few-shot prompting strategy has been suggested. Existing models struggle with detecting hallucination using standard prompting strategies. Other approaches rely on linking generated responses to facts from an external knowledge base. Linking to an external knowledge base is not always possible. Inspecting probabilities assigned to individual tokens can help detect hallucination. BARTScore estimates factuality by looking at conditional probabilities of generated text. Kadavath et al.',\n",
       " 'use a variation of the idea to estimate factuality. LLMs provide well-calibrated probabilities when answering multiple-choice questions. LLMs can convert answer validation into a multiple-choice question. A supervised classifier can be trained to predict whether an answer is true or false. Azaria and Mitchell (2023) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM. A supervised classifier can predict whether a given statement is true or not by using the weights from one of the hidden layers of the LLM. The approach of training a supervised classifier on the weights from one of the hidden layers of the LLM performs well. The need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.',\n",
       " 'Methods are needed when models do not provide access to token probabilities. SelfCheckGPT addresses the problem of not having access to token probabilities by sampling multiple answers. Factual answers are more stable and tend to be semantically similar when sampled multiple times. Hallucinated answers are less likely to be semantically similar when sampled multiple times. LLMs can be used to automatically evaluate the factuality of text generation systems. GPTScore uses a prompt that specifies the considered aspect and scores passages based on the average probability of the generated tokens. GPTScore considers aspects such as fluency when scoring passages. A prompt that specifies the considered aspect is used by GPTScore to score passages based on the average probability of the generated tokens. GPTScore uses a given autoregressive LM to score passages based on the average probability of the generated tokens. Using prompts was previously considered by Yuan et al.',\n",
       " '(2021) when evaluating aspects of generated text. Yuan et al.',\n",
       " '(2021) used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. Other approaches directly ask ChatGPT to evaluate a particular aspect of the given answer. ChatGPT can be asked to provide a score between 0 and 100 or provide a rating on a 5-star scale for a given answer. Providing a prompt is sensitive to the design of the prompt when directly asking ChatGPT to evaluate a particular aspect. Strong results can be obtained when directly asking ChatGPT to evaluate a particular aspect of the given answer. Scoring individual answers is not necessary, other approaches can be used to evaluate aspects of generated text. LLMs can be used to select the best answer among multiple candidates. Selecting the best answer among multiple candidates can be used to compare the performance of different LMs. Care is needed when using an LLM to select the best answer among multiple candidates. Different approaches can be used to automatically evaluate other aspects of generated text fragments beyond factuality. Several candidates exist to compare LLMs. Comparing LLMs requires care due to the order of answers. Different methods have been used in the literature to compare generated answers. Typically, one or more reference answers are used in these comparisons. BERTScore uses contextual embeddings to compare generated and reference answers. MoverScore uses contextual embeddings to compare generated and reference answers. BARTScore uses reference answers to estimate precision and recall. A standard RAG setting involves retrieving context and generating an answer. RAG systems involve retrieving context and generating an answer based on that context. We do not have access to human-annotated datasets or reference answers. We focus on metrics that are fully self-contained and reference-free. We focus on three quality aspects that are of central importance. Faithfulness refers to an answer being grounded in the given context. Faithfulness is important to avoid hallucinations and justify generated answers. Faithfulness is crucial in domains like law, where factual consistency is vital. Answer Relevance refers to the generated answer addressing the provided question. Context Relevance refers to the retrieved context being focused and containing little irrelevant information. Long context passages make LLMs less effective in exploiting context information. We can measure the three quality aspects in a fully automated way by prompting an LLM. We use the gpt-3.5-turbo-16k model to evaluate prompts through the OpenAI API. An answer is faithful if claims made in the answer can be inferred from the context.',\n",
       " 'To estimate faithfulness, we extract statements from the context using an LLM. We decompose longer sentences into shorter assertions to improve context understanding. We use a prompt to create one or more statements from each sentence in a given question and answer. We use the gpt-3.5-turbo-16k model for all prompts in our implementation and experiments. We use a prompt for this step. The prompt is: Given a question and answer, create one or more statements from each sentence in the given answer. question and answer refer to the given question and answer. For each statement Si, the LLM determines if Si can be inferred from c(q) using a verification function v(Si, c(q)). The verification function is carried out using the following prompt: Consider the given context and following statements. Then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format.',\n",
       " 'Do not deviate from the specified format. The final faithfulness score, F, is then computed as F = |V| / |S|. V is the number of statements that were supported according to the LLM.',\n",
       " 'S is the total number of statements. Answer relevance is We say that the answer as(q) is relevant if it directly addresses the question in an appropriate way. Our assessment of answer relevance does not take into account factuality. It penalises cases where the answer is incomplete or where it contains redundant information. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi. We generate a question for the given answer. We obtain embeddings for all questions using the text-embedding-ada-002 model. Available from the OpenAI API. For each qi, we calculate the similarity sim(q, qi) with the original question q. We also use a prompt for a demonstration as part of the step.',\n",
       " 'The prompt is: Given a question and answer, create one or more statements from each sentence in the given answer. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. In S, the LLM determines if Si can be inferred from c(q) using a verification function v(Si, c(q)). This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context.',\n",
       " 'Provide a brief explanation for each statement before arriving at the verdict (Yes/No).',\n",
       " 'Provide a final verdict for each statement in order at the end in the given format. We obtain embeddings for all questions using the text-embedding-ada-002 model. The text-embedding-ada-002 model is available from the OpenAI API. We calculate the similarity with the original question as the cosine between the corresponding embeddings. The answer relevance score for a question is computed as the sum of similarities with other questions. The answer relevance score is a metric that evaluates how closely the generated answer aligns with the initial question or instruction. The context exclusively contains information that is needed to answer the question. The context relevance metric aims to penalise the model when the context contains unnecessary information. Including redundant information is a problem. To estimate context relevance, given a question and its context, a language model extracts a subset of sentences. The language model extracts a subset of sentences that are crucial to answer the question. The language model extracts a subset of sentences without making any changes to the context. Context relevance is computed as the number of extracted sentences divided by the total number of sentences in the context. The WikiEval Dataset is proposed to evaluate a framework. The WikiEval Dataset includes question-context-answer triples annotated with human judgments. The WikiEval Dataset is used to verify the agreement between metrics and human assessments. A new dataset called WikiEval was created because no publicly available datasets met the required criteria. The WikiEval dataset includes 50 Wikipedia pages covering events that happened since the start of 2022. Recent Wikipedia pages with edits were prioritized when selecting the 50 pages. ChatGPT suggested a question based on the introductory section of each Wikipedia page. The question must be fully answered from the given context.',\n",
       " 'The question must not contain any links. The question must be framed from a part that contains non-trivial information.',\n",
       " 'The answer should not contain any links. The reported training cutoff of the model used in the experiment is 2022. The model used in the experiments is different from the one used in production. The answer should not contain any links.',\n",
       " 'The question should be of moderate difficulty.',\n",
       " \"The question must be reasonable and must be understood and responded to by humans. Do not use phrases that 'provided context', etc in the question We used ChatGPT to answer the generated question. We gave the corresponding introductory section as context. The prompt for answering was: Answer the question using the information from the given context. The questions were annotated by two annotators. Both annotators were fluent in English. The annotators received clear instructions about the considered quality dimensions. The annotators agreed in around 95% of cases on faithfulness and context relevance. The annotators agreed in around 90% of the cases on answer relevance. Disagreements were resolved after a discussion between the annotators. We used ChatGPT to answer the question without access to any additional context. Human annotators judged the faithfulness of the answers. We asked human annotators to compare incompletely answered questions. Additional sentences were added to the context by scraping back-links to the corresponding Wikipedia page. The question to obtain lower answer relevance was: Answer the given question in an incomplete manner. Context relevance can be measured. Additional sentences were added to the context. The added sentences were scraped back-links from the corresponding Wikipedia page. The sentences added to the context were related. The additional information added to the context was less relevant. Faith, answer relevance, and context relevance are important quality dimensions. Ragas is a metric proposed for evaluating faithfulness, answer relevance, and context relevance. GPT Score is a baseline method for evaluating faithfulness, answer relevance, and context relevance. GPT Ranking is a baseline method for selecting the preferred answer/context. The WikEval dataset is a collection of human assessments for evaluating faithfulness, answer relevance, and context relevance. ChatGPT is used to complete the given context for pages without back-links. The accuracy of the proposed metrics is reported in terms of the fraction of instances on which the model agrees with human annotators. The proposed metrics, Ragas, are compared with two baseline methods, GPT Score and GPT Ranking, for evaluating faithfulness, answer relevance, and context relevance. Faithfulness measures the information consistency of the answer against the given context. Faithfulness should penalize claims that cannot be deduced from the context. Answer relevance measures the relevance of the answer to the given context. Context relevance measures the relevance of the context to the given answer. The proposed metrics, Ragas, have scores of 0.95, 0.78, and 0.70 for faithfulness, answer relevance, and context relevance, respectively. GPT Score has scores of 0.72, 0.52, and 0.63 for faithfulness, answer relevance, and context relevance, respectively. GPT Ranking has scores of 0.54, 0.40, and 0.52 for faithfulness, answer relevance, and context relevance, respectively. There are five experiments to analyze the agreement between the proposed metrics and human assessments. Each WikiEval instance requires the model to compare two answers or two context fragments. A prompt is used to ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions. Ties in the scores assigned by the LLM are broken randomly. The second baseline was shown as GPT Ranking. The second baseline asks ChatGPT to select the preferred answer/context. A definition of the considered quality metric was included in the prompt. A prompt was used to evaluate answer relevance. Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. Answer Relevancy penalizes the presence of redundant information or incomplete answers given a question. Rank each answer based on Answer Relevancy. Our proposed metrics are much closer aligned with human judgements than the predictions from the two baseline models. The Ragas prediction are highly accurate for faithfulness. The agreement is lower for answer relevance.\",\n",
       " 'The differences between the two candidate answers are often very subtle. Context relevance is the hardest quality dimension to evaluate. ChatGPT often struggles with selecting the sentences from the context that are crucial, especially for longer contexts. Automated reference-free evaluation of RAG systems is needed. An evaluation framework should assess faithfulness, answer relevance, and context relevance. WikiEval is a dataset with human judgements of three different aspects. Ragas is a framework that supports the development of a quality assessment framework. Ragas is easy to use and can provide developers of RAG systems with feedback. The second baseline, GPT Ranking, was used to compare with Ragas. Broken ranking system is a reference that was broken randomly. The text describes three different aspects. Ragas is the implementation of the three considered quality aspects. This framework is easy to use. This framework can provide developers of RAG systems with valuable insights even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from Ragas are closely aligned with human predictions. The predictions from Ragas are closely aligned with human predictions for faithfulness. The predictions from Ragas are closely aligned with human predictions for answer relevance. Amos Azaria and Tom M. Mitchell published the paper The internal state of an LLM knows when its lying. The paper The internal state of an LLM knows when its lying has the abs/2304.13734 reference. Sebastian Borgeaud authored the paper Improving language models by retrieving from trillions of tokens. Arthur Mensch co-authored the paper Improving language models by retrieving from trillions of tokens. Jordan Hoffmann co-authored the paper Improving language models by retrieving from trillions of tokens. Trevor Cai co-authored the paper Improving language models by retrieving from trillions of tokens. Eliza Rutherford co-authored the paper Improving language models by retrieving from trillions of tokens. Katie Millican co-authored the paper Improving language models by retrieving from trillions of tokens. George van den Driessche co-authored the paper Improving language models by retrieving from trillions of tokens. Jean-Baptiste Lespiau co-authored the paper Improving language models by retrieving from trillions of tokens. Bogdan Damoc co-authored the paper Improving language models by retrieving from trillions of tokens. Aidan Clark co-authored the paper Improving language models by retrieving from trillions of tokens. Diego de Las Casas co-authored the paper Improving language models by retrieving from trillions of tokens. Aurelia Guy co-authored the paper Improving language models by retrieving from trillions of tokens. Jacob Menick co-authored the paper Improving language models by retrieving from trillions of tokens. Roman Ring co-authored the paper Improving language models by retrieving from trillions of tokens. Tom Hennigan co-authored the paper Improving language models by retrieving from trillions of tokens. Saffron Huang co-authored the paper Improving language models by retrieving from trillions of tokens. Loren Maggiore co-authored the paper Improving language models by retrieving from trillions of tokens. Chris Jones co-authored the paper Improving language models by retrieving from trillions of tokens. Albin Cassirer co-authored the paper Improving language models by retrieving from trillions of tokens. Andy Brock co-authored the paper Improving language models by retrieving from trillions of tokens. Michela Paganini co-authored the paper Improving language models by retrieving from trillions of tokens. Geoffrey Irving co-authored the paper Improving language models by retrieving from trillions of tokens. Oriol Vinyals co-authored the paper Improving language models by retrieving from trillions of tokens. Simon Osindero co-authored the paper Improving language models by retrieving from trillions of tokens. Karen Simonyan co-authored the paper Improving language models by retrieving from trillions of tokens. Jack W. Rae co-authored the paper Improving language models by retrieving from trillions of tokens. Erich Elsen co-authored the paper Improving language models by retrieving from trillions of tokens. Laurent Sifre co-authored the paper Improving language models by retrieving from trillions of tokens. The paper Improving language models by retrieving from trillions of tokens was published in International Conference on Machine Learning, ICML 2022. The paper Improving language models by retrieving from trillions of tokens was published in volume 162 of Proceedings of Machine Learning Research. Sébastien Bubeck and co-authors published the paper Sparks of artificial general intelligence: Early experiments with gpt-4. The paper Sparks of artificial general intelligence: Early experiments with gpt-4 has the arXiv preprint arXiv:2303.12712 reference. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova authored the paper BERT: Pre-training of deep bidirectional transformers for language understanding. The paper BERT: Pre-training of deep bidirectional transformers for language understanding was published in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu published the paper Gptscore: Evaluate as you desire. The paper Gptscore: Evaluate as you desire has the CoRR reference abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang authored the paper Retrieval augmented language model pre-training. The paper Retrieval augmented language model pre-training was published in International conference on machine learning. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung authored the paper Survey of hallucinations and reasoning in transformers. The authors of ACM Computing Surveys, 55(12):1–38 are Ziwei Ji, Nayeon Lee, Rita Frieske, etc. The title of ACM Computing Surveys, 55(12):1–38 is Survey of hallucination in natural language generation The authors of CoRR, abs/2207.05221 are Saurav Kadavath, Tom Conerly, Amanda Askell, etc. The title of CoRR, abs/2207.05221 is Language models (mostly) know what they know The date of CoRR, abs/2207.05221 is 2022 The authors of CoRR, abs/2211.08411 are Nikhil Kandpal, Haikang Deng, Adam Roberts, etc. The title of CoRR, abs/2211.08411 is Large language models struggle to learn long-tail knowledge The date of CoRR, abs/2211.08411 is 2022 The authors of Generalization through memorization: Nearest neighbor language models are Urvashi Khandelwal, Omer Levy, etc. The title of Generalization through memorization: Nearest neighbor language models in 8th International Conference on Learning Representations, ICLR 2020 is given The location of 8th International Conference on Learning Representations, ICLR 2020 is Addis Ababa, Ethiopia The date of 8th International Conference on Learning Representations, ICLR 2020 is April 26-30, 2020 The authors of Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP are Omar Khattab, Keshav Santhanam, etc. The title of Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP is Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP The date of Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP is 2022 The authors ofLatent retrieval for weakly supervised open domain question answering are Kenton Lee, Ming-Wei Chang, etc. The title ofLatent retrieval for weakly supervised open domain question answering is Latent retrieval for weakly supervised open domain question answering The year ofLatent retrieval for weakly supervised open domain question answering is 2019 The authors of Language models (mostly) know what they know are Saurav Kadavath, Tom Conerly, Amanda Askell, etc. The venue of Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics is Annual Meeting of the Association for Computational Linguistics The year of Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics is 2019 The authors of Generalization through memorization: Nearest neighbor language models are Urvashi Khandelwal, Omer Levy, etc. The year of Generalization through memorization: Nearest neighbor language models is 2020 The authors of Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP are Omar Khattab, Keshav Santhanam, etc. The year of 8th International Conference on Learning Representations, ICLR 2020 is 2020 The authors of Latent retrieval for weakly supervised open domain question answering are Kenton Lee, Ming-Wei Chang, etc. The year of Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP is 2022 Patrick S. H. Lewis is the author of the 2020 paper titled Retrieval-augmented generation for knowledge-intensive NLP tasks. The 2020 paper titled Retrieval-augmented generation for knowledge-intensive NLP tasks was presented in Advances in Neural Information Processing Systems 33. The 2020 paper titled Retrieval-augmented generation for knowledge-intensive NLP tasks was presented at the Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020. The 2020 paper titled Retrieval-augmented generation for knowledge-intensive NLP tasks took place on December 6-12, 2020, virtually. Junyi Li is the author of the 2023 paper titled Halueval: A large-scale hallucination evaluation benchmark for large language models. Halueval: A large-scale hallucination evaluation benchmark for large language models was presented in CoRR. The 2023 paper titled Halueval: A large-scale hallucination evaluation benchmark for large language models has the ABS number 2305.11747. Nelson F. Liu is the author of the 2023 paper titled Lost in the middle: How language models use long contexts. The 2023 paper titled Lost in the middle: How language models use long contexts was presented in an anonymous source. Alex Mallen is the author of the 2023 paper titled When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories was presented in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics The 2023 paper titled When not to trust language models: Investigating effectiveness of parametric and non-parametric memories has the volume 1: Long Papers and pages 9802–9822. The 2023 paper titled When not to trust language models: Investigating effectiveness of parametric and non-parametric memories took place in Toronto, Canada. The 2023 paper titled When not to trust language models: Investigating effectiveness of parametric and non-parametric memories was published by the Association for Computational Linguistics. Potsawee Manakul is the author of the 2023 paper titled Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models was presented in CoRR. The 2023 paper titled Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models has the ABS number 2303.08896. Sewon Min is the author of the 2023 paper titled Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation was presented in CoRR. The 2023 paper titled Factscore: Fine-grained atomic evaluation of factual precision in long form text generation has the ABS number 2305.14251. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham published In-context retrieval-augmented language models in 2023. Adam Roberts, Colin Raffel, and Noam Shazeer published How much knowledge can you pack into the parameters of a language model? in 2020. The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) took place in 2020. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham published In-context retrieval-augmented language models in CoRR, abs/2302.00083. Adam Roberts, Colin Raffel, and Noam Shazeer published How much knowledge can you pack into the parameters of a language model? in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih published REPLUG: retrieval-augmented black-box language models in CoRR, abs/2301.12652. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou published Is chatgpt a good NLG evaluator? A preliminary study in CoRR, abs/2303.04048. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui published Large language models are not fair evaluators in CoRR, abs/2305.17926. Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer published KNN-LM does not improve open-ended text generation in CoRR, abs/2305.14625. Weizhe Yuan, Graham Neubig, and Pengfei Liu published Bartscore: Evaluating generated text as text generation in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 27263–27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass published Interpretable unified language checking in CoRR, abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi published Bertscore: Evaluating text generation with BERT in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. The text generation is with BERT The event is ICLR 2020 The conference is held in Addis Ababa, Ethiopia The conference is held from April 26-30, 2020 The conference is hosted by OpenRe-view.net Wei Zhao is an author Maxime Peyrard is an author Fei Liu is an author Yang Gao is an author Christian M. Meyer is an author Steffen Eger is an author The paper is MoverScore The paper evaluates text generation with contextualized embeddings and earth mover distance The paper was published in EMNLP-IJCNLP 2019 The paper was published in Association for Computational Linguistics The paper was published in Hong Kong, China There are example results from WikiEval The examples are shown in Tables 2, 3 and 4 The examples show answers with high and low faithfulness The examples show answers with high and low answer relevance The examples show answers with high and low context relevance Oppenheimer is a 2023 biographical thriller film the film Oppenheimer is written and directed by Christopher Nolan the film is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin J.',\n",
       " \"Robert Oppenheimer was a theoretical physicist J. Robert Oppenheimer was pivotal in developing the first nuclear weapons as part of the Manhattan Project the Manhattan Project ushered in the Atomic Age Cillian Murphy stars as Oppenheimer Emily Blunt stars as Katherine 'Kitty' Oppenheimer Christopher Nolan directed the film Oppenheimer Cillian Murphy stars as J. Robert Oppenheimer in the film the PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC the PSLV-C56 mission will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India J. Robert Oppenheimer was the first wife of Sayajirao Gaekwad III of Baroda State the Chimnabai Clock Tower was completed in 1896 the Chimnabai Clock Tower is a clock tower situated in the Raopura area of Vadodara, Gujarat, India the Chimnabai Clock Tower was named in memory of Chimnabai I (1864–1885) the PSLV-C56 mission is an important space mission for India the PSLV-C56 mission aims to launch a satellite into orbit to study weather patterns James Cameron did not direct the film Oppenheimer Tom Cruise does not star as J. Robert Oppenheimer in the film The Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I. Chimnabai I was a queen. Chimnabai I was the first wife of Sayajirao Gaekwad III. Sayajirao Gaekwad III was of Baroda State. The Chimnabai Clock Tower is in the Raopura area of Vadodara, Gujarat, India. The Chimnabai Clock Tower was built in Indo-Saracenic architecture style. Mir Kamaluddin Hussainkhan inaugurated the Chimnabai Clock Tower. The Chimnabai Clock Tower was a stoppage for horse drawn trams. The Chimnabai Clock Tower was built at the cost of 25,000. 25,000 is equivalent to 9.2 million or USD 120,000 in 2023.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8fcabea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Ragas\", \"is\", \"framework\"],\n",
      "       [\"Ragas\", \"provides\", \"reference-free evaluation\"],\n",
      "       [\"RAG systems\", \"are composed of\", \"retrieval module\"],\n",
      "       [\"RAG systems\", \"are composed of\", \"LLM based generation module\"],\n",
      "       [\"RAG systems\", \"provide\", \"knowledge\"],\n",
      "       [\"RAG systems\", \"reduce\", \"risk of hallucinations\"],\n",
      "       [\"RAG architectures\", \"are challenging to evaluate\", \"because of several dimensions\"],\n",
      "       [\"RAG architectures\", \"consider\", \"ability to identify relevant context passages\"],\n",
      "       [\"RAG architectures\", \"consider\", \"ability to exploit passages\"],\n",
      "       [\"RAG architectures\", \"consider\", \"quality of generation\"],\n",
      "       [\"Ragas\", \"provides\", \"suite of metrics\"],\n",
      "       [\"Ragas\", \"contributes to\", \"faster evaluation cycles\"],\n",
      "       [\"RAG architectures\", \"are adopted quickly\", \"due to fast adoption of LLMs\"],\n",
      "       [\"Language Models\", \"capture\", \"knowledge about the world\"],\n",
      "       [\"Language Models\", \"answer\", \"questions\"],\n",
      "       [\"LMs\", \"emerged as\", \"repositories of knowledge\"],\n",
      "       [\"Introduction of BERT\", \"established\", \"LMs as repositories of knowledge\"],\n",
      "       [\"Recent Large Language Models\", \"capture\", \"knowledge to rival human performance\"],\n",
      "       [\"Large Language Models\", \"achieve\", \"human performance\"],\n",
      "       [\"Fast adoption of LLMs\", \"is driven by\", \"need for faster evaluation cycles\"],\n",
      "       [\"LLMs\", \"have\", \"knowledge to rival human performance\"],\n",
      "       [\"LLMs\", \"are not able to answer\", \"questions about recent events\"],\n",
      "       [\"Retrieval Augmented Generation\", \"is\", \"standard solution\"],\n",
      "       [\"RAG\", \"involves\", \"retrieving relevant passages\"],\n",
      "       [\"RAG\", \"requires\", \"significant amount of tuning\"],\n",
      "       [\"Retrieval-augmented systems\", \"are affected by\", \"retrieval model\"],\n",
      "       [\"Retrieval-augmented systems\", \"are affected by\", \"considered corpus\"],\n",
      "       [\"Retrieval-augmented systems\", \"are affected by\", \"LM\"],\n",
      "       [\"Retrieval-augmented systems\", \"are affected by\", \"prompt formulation\"],\n",
      "       [\"Automated evaluation\", \"is\", \"paramount\"],\n",
      "       [\"RAG systems\", \"are evaluated\", \"in terms of language modelling task\"],\n",
      "       [\"Evaluation strategy\", \"relies on\", \"LM probabilities\"],\n",
      "       [\"LM probabilities\", \"are not accessible\", \"for some language models\"],\n",
      "       [\"Ragas\", \"is\", \"framework for automated assessment\"],\n",
      "       [\"Ragas\", \"is available at\", \"https://github.com/explodinggradients/ragas\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:   6%|▌         | 1/18 [00:05<01:34,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Ragas framework\", \"integrates with\", \"llama-index\"],\n",
      "       [\"Ragas framework\", \"integrates with\", \"Langchain\"],\n",
      "       [\"llama-index\", \"is used for\", \"building RAG solutions\"],\n",
      "       [\"Langchain\", \"is used for\", \"building RAG solutions\"],\n",
      "       [\"Ragas\", \"can be integrated into\", \"standard workflows\"],\n",
      "       [\"LLMs\", \"are used for\", \"estimating faithfulness\"],\n",
      "       [\"LLMs\", \"struggle with\", \"detecting hallucination\"],\n",
      "       [\"standard prompting strategies\", \"are used for\", \"detecting hallucination\"],\n",
      "       [\"external knowledge base\", \"is used for\", \"linking generated responses\"],\n",
      "       [\"BARTScore\", \"estimates\", \"factuality\"],\n",
      "       [\"BARTScore\", \"looks at\", \"conditional probabilities of generated text\"],\n",
      "       [\"Kadavath et al.\", \"studied\", \"detecting hallucination\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  11%|█         | 2/18 [00:06<00:44,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"LLMs\", \"provide\", \"well-calibrated probabilities\"],\n",
      "       [\"LLMs\", \"convert\", \"answer validation into a multiple-choice question\"],\n",
      "       [\"supervised classifier\", \"can be trained to predict\", \"whether an answer is true or false\"],\n",
      "       [\"Azaria and Mitchell\", \"propose\", \"training a supervised classifier on the weights from one of the hidden layers of the LLM\"],\n",
      "       [\"supervised classifier\", \"can predict\", \"whether a given statement is true or not\"],\n",
      "       [\"supervised classifier\", \"uses\", \"weights from one of the hidden layers of the LLM\"],\n",
      "       [\"approach\", \"performs well\", \"training a supervised classifier on the weights from one of the hidden layers of the LLM\"],\n",
      "       [\"approach\", \"is unsuitable for\", \"systems that access LLMs through an API\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  17%|█▋        | 3/18 [00:07<00:28,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Methods\", \"are needed for\", \"models\"],\n",
      "       [\"SelfCheckGPT\", \"addresses\", \"problem of token probabilities\"],\n",
      "       [\"Factual answers\", \"are\", \"semantically similar\"],\n",
      "       [\"Hallucinated answers\", \"are not\", \"semantically similar\"],\n",
      "       [\"LLMs\", \"can be used for\", \"evaluation of text generation systems\"],\n",
      "       [\"GPTScore\", \"uses\", \"prompt\"],\n",
      "       [\"GPTScore\", \"scores\", \"passages\"],\n",
      "       [\"GPTScore\", \"considers\", \"fluency\"],\n",
      "       [\"GPTScore\", \"uses\", \"autoregressive LM\"],\n",
      "       [\"Yuan et al.\", \"considered\", \"using prompts\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  28%|██▊       | 5/18 [00:08<00:13,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Yuan et al.\", \"evaluates\", \"aspects of generated text\"],\n",
      "       [\"Yuan et al.\", \"performed evaluation in\", \"2021\"]\n",
      "   ]\n",
      "}\n",
      "{\n",
      "  \"triplets\": [\n",
      "       [\"2021\", \"used\", \"BART\"],\n",
      "       [\"ChatGPT\", \"evaluates\", \"given answer\"],\n",
      "       [\"ChatGPT\", \"provides\", \"score\"],\n",
      "       [\"ChatGPT\", \"provides\", \"rating\"],\n",
      "       [\"Prompt\", \"is sensitive to\", \"design\"],\n",
      "       [\"ChatGPT\", \"evaluates\", \"particular aspect\"],\n",
      "       [\"LLMs\", \"select\", \"best answer\"],\n",
      "       [\"LLMs\", \"compare\", \"performance\"],\n",
      "       [\"LLMs\", \"require\", \"care\"],\n",
      "       [\"Approaches\", \"evaluate\", \"generated text fragments\"],\n",
      "       [\"BERTScore\", \"compares\", \"generated answers\"],\n",
      "       [\"MoverScore\", \"compares\", \"generated answers\"],\n",
      "       [\"BARTScore\", \"estimates\", \"precision and recall\"],\n",
      "       [\"RAG systems\", \"retrieve\", \"context\"],\n",
      "       [\"RAG systems\", \"generate\", \"answer\"],\n",
      "       [\"Metrics\", \"are\", \"self-contained\"],\n",
      "       [\"Metrics\", \"are\", \"reference-free\"],\n",
      "       [\"Faithfulness\", \"refers to\", \"answer being grounded\"],\n",
      "       [\"Faithfulness\", \"avoids\", \"hallucinations\"],\n",
      "       [\"Faithfulness\", \"justifies\", \"generated answers\"],\n",
      "       [\"Answer Relevance\", \"refers to\", \"generated answer\"],\n",
      "       [\"Context Relevance\", \"refers to\", \"retrieved context\"],\n",
      "       [\"LLMs\", \"are less effective with\", \"long context passages\"],\n",
      "       [\"LLMs\", \"measure\", \"quality aspects\"],\n",
      "       [\"gpt-3.5-turbo-16k model\", \"evaluates\", \"prompts\"],\n",
      "       [\"OpenAI API\", \"provides\", \"evaluation\"],\n",
      "       [\"Answer\", \"is faithful if\", \"claims can be inferred from context\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  39%|███▉      | 7/18 [00:10<00:11,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"LLM\", \"is used for\", \"faithfulness estimation\"],\n",
      "       [\"Sentences\", \"are decomposed into\", \"shorter assertions\"],\n",
      "       [\"Prompt\", \"is used to create\", \"statements\"],\n",
      "       [\"gpt-3.5-turbo-16k model\", \"is used for\", \"prompts\"],\n",
      "       [\"LLM\", \"determines inference of\", \"statements\"],\n",
      "       [\"Verification function\", \"is carried out using\", \"prompt\"],\n",
      "       [\"Context\", \"is used for\", \"verification\"],\n",
      "       [\"Statements\", \"are verified against\", \"context\"],\n",
      "       [\"Verification function\", \"provides\", \"explanation and verdict\"]\n",
      "   ]\n",
      "}\n",
      "{\n",
      "  \"triplets\": [\n",
      "       [\"F\", \"is computed as\", \"faithfulness score\"],\n",
      "       [\"F\", \"equals\", \"|V| / |S|\"],\n",
      "       [\"V\", \"is\", \"number of statements\"],\n",
      "       [\"V\", \"is supported by\", \"LLM\"],\n",
      "       [\"F\", \"is calculated using\", \"V and S\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  44%|████▍     | 8/18 [00:11<00:09,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Answer relevance\", \"is assessed by\", \"Our assessment\"],\n",
      "       [\"Our assessment\", \"penalises\", \"incomplete answers\"],\n",
      "       [\"Our assessment\", \"penalises\", \"redundant information\"],\n",
      "       [\"LLM\", \"generates\", \"potential questions\"],\n",
      "       [\"Text-embedding-ada-002 model\", \"is obtained from\", \"OpenAI API\"],\n",
      "       [\"Text-embedding-ada-002 model\", \"is used for\", \"question embeddings\"],\n",
      "       [\"Similarity\", \"is calculated between\", \"original question and potential questions\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  56%|█████▌    | 10/18 [00:12<00:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"LLM\", \"determines\", \"inference of Si\"],\n",
      "       [\"LLM\", \"uses\", \"verification function v\"],\n",
      "       [\"Verification function v\", \"takes\", \"Si and c(q)\"],\n",
      "       [\"Verification step\", \"is carried out using\", \"prompt\"],\n",
      "       [\"Prompt\", \"considers\", \"given context and statements\"],\n",
      "       [\"Prompt\", \"determines\", \"support of statements by context\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  61%|██████    | 11/18 [00:13<00:04,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Text\", \"requires\", \"explanation\"],\n",
      "       [\"Explanation\", \"is provided for\", \"statement\"],\n",
      "       [\"Statement\", \"receives\", \"verdict\"],\n",
      "       [\"Verdict\", \"is given as\", \"Yes/No\"]\n",
      "   ]\n",
      "}\n",
      "{\n",
      "  \"triplets\": [\n",
      "       [\"text-embedding-ada-002 model\", \"is available from\", \"OpenAI API\"],\n",
      "       [\"embeddings\", \"are obtained for\", \"questions\"],\n",
      "       [\"similarity\", \"is calculated as\", \"cosine between embeddings\"],\n",
      "       [\"answer relevance score\", \"is computed as\", \"sum of similarities\"],\n",
      "       [\"answer relevance score\", \"evaluates\", \"generated answer\"],\n",
      "       [\"context\", \"contains\", \"information\"],\n",
      "       [\"context relevance metric\", \"penalises\", \"model\"],\n",
      "       [\"language model\", \"extracts\", \"subset of sentences\"],\n",
      "       [\"context relevance\", \"is computed as\", \"number of extracted sentences divided by total number of sentences\"],\n",
      "       [\"WikiEval Dataset\", \"is proposed to evaluate\", \"framework\"],\n",
      "       [\"WikiEval Dataset\", \"includes\", \"question-context-answer triples\"],\n",
      "       [\"WikiEval Dataset\", \"is used to verify\", \"agreement between metrics and human assessments\"],\n",
      "       [\"WikiEval dataset\", \"includes\", \"50 Wikipedia pages\"],\n",
      "       [\"Wikipedia pages\", \"were selected based on\", \"edits\"],\n",
      "       [\"ChatGPT\", \"suggests\", \"question\"],\n",
      "       [\"question\", \"is based on\", \"introductory section of Wikipedia page\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  72%|███████▏  | 13/18 [00:15<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"question\", \"must not contain\", \"links\"],\n",
      "       [\"question\", \"must be framed from\", \"part\"],\n",
      "       [\"part\", \"contains\", \"non-trivial information\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  78%|███████▊  | 14/18 [00:15<00:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"model\", \"has training cutoff\", \"2022\"],\n",
      "       [\"model used in experiment\", \"is different from\", \"model used in production\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  83%|████████▎ | 15/18 [00:15<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"question\", \"should be of\", \"moderate difficulty\"]\n",
      "   ]\n",
      "}\n",
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Question\", \"must be understood by\", \"humans\"],\n",
      "       [\"ChatGPT\", \"answers\", \"generated question\"],\n",
      "       [\"Introductory section\", \"is used as\", \"context\"],\n",
      "       [\"Prompt\", \"instructs to answer\", \"question\"],\n",
      "       [\"Annotators\", \"annotate\", \"questions\"],\n",
      "       [\"Annotators\", \"are fluent in\", \"English\"],\n",
      "       [\"Annotators\", \"receive\", \"instructions\"],\n",
      "       [\"Annotators\", \"agree on\", \"faithfulness\"],\n",
      "       [\"Annotators\", \"agree on\", \"context relevance\"],\n",
      "       [\"Disagreements\", \"are resolved by\", \"discussion\"],\n",
      "       [\"ChatGPT\", \"answers question without\", \"additional context\"],\n",
      "       [\"Human annotators\", \"judge\", \"faithfulness\"],\n",
      "       [\"Human annotators\", \"compare\", \"incompletely answered questions\"],\n",
      "       [\"Context\", \"is added with\", \"Wikipedia page\"],\n",
      "       [\"Additional sentences\", \"are scraped from\", \"Wikipedia page\"],\n",
      "       [\"Additional sentences\", \"are related to\", \"context\"],\n",
      "       [\"Additional information\", \"is less relevant to\", \"context\"],\n",
      "       [\"Faithfulness\", \"is measured by\", \"Ragas\"],\n",
      "       [\"Answer relevance\", \"is measured by\", \"Ragas\"],\n",
      "       [\"Context relevance\", \"is measured by\", \"Ragas\"],\n",
      "       [\"GPT Score\", \"is baseline method for\", \"faithfulness\"],\n",
      "       [\"GPT Score\", \"is baseline method for\", \"answer relevance\"],\n",
      "       [\"GPT Score\", \"is baseline method for\", \"context relevance\"],\n",
      "       [\"GPT Ranking\", \"is baseline method for\", \"faithfulness\"],\n",
      "       [\"GPT Ranking\", \"is baseline method for\", \"answer relevance\"],\n",
      "       [\"GPT Ranking\", \"is baseline method for\", \"context relevance\"],\n",
      "       [\"WikEval dataset\", \"is collection of\", \"human assessments\"],\n",
      "       [\"ChatGPT\", \"completes context for\", \"pages without back-links\"],\n",
      "       [\"Ragas\", \"is compared to\", \"GPT Score\"],\n",
      "       [\"Ragas\", \"is compared to\", \"GPT Ranking\"],\n",
      "       [\"Faithfulness\", \"measures\", \"information consistency\"],\n",
      "       [\"Answer relevance\", \"measures\", \"relevance of answer\"],\n",
      "       [\"Context relevance\", \"measures\", \"relevance of context\"],\n",
      "       [\"Ragas\", \"has scores for\", \"faithfulness\"],\n",
      "       [\"Ragas\", \"has scores for\", \"answer relevance\"],\n",
      "       [\"Ragas\", \"has scores for\", \"context relevance\"],\n",
      "       [\"GPT Score\", \"has scores for\", \"faithfulness\"],\n",
      "       [\"GPT Score\", \"has scores for\", \"answer relevance\"],\n",
      "       [\"GPT Score\", \"has scores for\", \"context relevance\"],\n",
      "       [\"GPT Ranking\", \"has scores for\", \"faithfulness\"],\n",
      "       [\"GPT Ranking\", \"has scores for\", \"answer relevance\"],\n",
      "       [\"GPT Ranking\", \"has scores for\", \"context relevance\"],\n",
      "       [\"Experiments\", \"analyze agreement between\", \"Ragas and human assessments\"],\n",
      "       [\"WikiEval instance\", \"requires model to compare\", \"answers\"],\n",
      "       [\"Prompt\", \"asks ChatGPT to assign\", \"score\"],\n",
      "       [\"Ties\", \"are broken randomly by\", \"LLM\"],\n",
      "       [\"GPT Ranking\", \"asks ChatGPT to select\", \"preferred answer\"],\n",
      "       [\"Prompt\", \"includes definition of\", \"quality metric\"],\n",
      "       [\"Prompt\", \"evaluates\", \"answer relevance\"],\n",
      "       [\"Answer Relevancy\", \"measures\", \"degree of response\"],\n",
      "       [\"Answer Relevancy\", \"penalizes\", \"redundant information\"],\n",
      "       [\"Ragas\", \"is aligned with\", \"human judgements\"],\n",
      "       [\"Ragas prediction\", \"is accurate for\", \"faithfulness\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  89%|████████▉ | 16/18 [00:19<00:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Ragas\", \"supports\", \"development of quality assessment framework\"],\n",
      "       [\"Ragas\", \"is\", \"framework\"],\n",
      "       [\"Ragas\", \"provides\", \"feedback to developers of RAG systems\"],\n",
      "       [\"Ragas\", \"is easy to use\", \"use\"],\n",
      "       [\"GPT Ranking\", \"is\", \"baseline\"],\n",
      "       [\"Broken ranking system\", \"is\", \"reference\"],\n",
      "       [\"WikiEval\", \"is\", \"dataset\"],\n",
      "       [\"WikiEval\", \"contains\", \"human judgements\"],\n",
      "       [\"Evaluation framework\", \"should assess\", \"faithfulness\"],\n",
      "       [\"Evaluation framework\", \"should assess\", \"answer relevance\"],\n",
      "       [\"Evaluation framework\", \"should assess\", \"context relevance\"],\n",
      "       [\"Ragas\", \"predicts\", \"faithfulness\"],\n",
      "       [\"Ragas\", \"predicts\", \"answer relevance\"],\n",
      "       [\"Ragas\", \"predicts\", \"context relevance\"],\n",
      "       [\"Amos Azaria\", \"published\", \"The internal state of an LLM knows when its lying\"],\n",
      "       [\"Tom M. Mitchell\", \"published\", \"The internal state of an LLM knows when its lying\"],\n",
      "       [\"Sebastian Borgeaud\", \"authored\", \"Improving language models by retrieving from trillions of tokens\"],\n",
      "       [\"Arthur Mensch\", \"co-authored\", \"Improving language models by retrieving from trillions of tokens\"],\n",
      "       [\"Jacob Devlin\", \"authored\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\"],\n",
      "       [\"Jinlan Fu\", \"published\", \"Gptscore: Evaluate as you desire\"],\n",
      "       [\"Kelvin Guu\", \"authored\", \"Retrieval augmented language model pre-training\"],\n",
      "       [\"Ziwei Ji\", \"authored\", \"Survey of hallucinations and reasoning in transformers\"],\n",
      "       [\"Saurav Kadavath\", \"authored\", \"Language models (mostly) know what they know\"],\n",
      "       [\"Urvashi Khandelwal\", \"authored\", \"Generalization through memorization: Nearest neighbor language models\"],\n",
      "       [\"Omar Khattab\", \"authored\", \"Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP\"],\n",
      "       [\"Kenton Lee\", \"authored\", \"Latent retrieval for weakly supervised open domain question answering\"],\n",
      "       [\"Patrick S. H. Lewis\", \"authored\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\"],\n",
      "       [\"Junyi Li\", \"authored\", \"Halueval: A large-scale hallucination evaluation benchmark for large language models\"],\n",
      "       [\"Nelson F. Liu\", \"authored\", \"Lost in the middle: How language models use long contexts\"],\n",
      "       [\"Alex Mallen\", \"authored\", \"When not to trust language models: Investigating effectiveness of parametric and non-parametric memories\"],\n",
      "       [\"Potsawee Manakul\", \"authored\", \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\"],\n",
      "       [\"Sewon Min\", \"authored\", \"Factscore: Fine-grained atomic evaluation of factual precision in long form text generation\"],\n",
      "       [\"Ori Ram\", \"published\", \"In-context retrieval-augmented language models\"],\n",
      "       [\"Adam Roberts\", \"published\", \"How much knowledge can you pack into the parameters of a language model?\"],\n",
      "       [\"Weijia Shi\", \"published\", \"REPLUG: retrieval-augmented black-box language models\"],\n",
      "       [\"Jiaan Wang\", \"published\", \"Is chatgpt a good NLG evaluator? A preliminary study\"],\n",
      "       [\"Peiyi Wang\", \"published\", \"Large language models are not fair evaluators\"],\n",
      "       [\"Shufan Wang\", \"published\", \"KNN-LM does not improve open-ended text generation\"],\n",
      "       [\"Weizhe Yuan\", \"published\", \"Bartscore: Evaluating generated text as text generation\"],\n",
      "       [\"Tianhua Zhang\", \"published\", \"Interpretable unified language checking\"],\n",
      "       [\"Tianyi Zhang\", \"published\", \"Bertscore: Evaluating text generation with BERT\"],\n",
      "       [\"Wei Zhao\", \"is\", \"author\"],\n",
      "       [\"Maxime Peyrard\", \"is\", \"author\"],\n",
      "       [\"Fei Liu\", \"is\", \"author\"],\n",
      "       [\"Yang Gao\", \"is\", \"author\"],\n",
      "       [\"Christian M. Meyer\", \"is\", \"author\"],\n",
      "       [\"Steffen Eger\", \"is\", \"author\"],\n",
      "       [\"MoverScore\", \"evaluates\", \"text generation\"],\n",
      "       [\"Oppenheimer\", \"is\", \"biographical thriller film\"],\n",
      "       [\"Christopher Nolan\", \"wrote and directed\", \"Oppenheimer\"],\n",
      "       [\"Kai Bird\", \"wrote\", \"American Prometheus\"],\n",
      "       [\"Martin J. Sherwin\", \"wrote\", \"American Prometheus\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j:  94%|█████████▍| 17/18 [00:22<00:01,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"triplets\": [\n",
      "       [\"Robert Oppenheimer\", \"was\", \"theoretical physicist\"],\n",
      "       [\"J. Robert Oppenheimer\", \"was pivotal in developing\", \"first nuclear weapons\"],\n",
      "       [\"Manhattan Project\", \"ushered in\", \"Atomic Age\"],\n",
      "       [\"Cillian Murphy\", \"stars as\", \"Oppenheimer\"],\n",
      "       [\"Emily Blunt\", \"stars as\", \"Katherine 'Kitty' Oppenheimer\"],\n",
      "       [\"Christopher Nolan\", \"directed\", \"film Oppenheimer\"],\n",
      "       [\"PSLV-C56 mission\", \"is scheduled to be launched on\", \"Sunday, 30 July 2023\"],\n",
      "       [\"PSLV-C56 mission\", \"will be launched from\", \"Satish Dhawan Space Centre\"],\n",
      "       [\"Satish Dhawan Space Centre\", \"is located in\", \"Sriharikota, Andhra Pradesh, India\"],\n",
      "       [\"Chimnabai Clock Tower\", \"was completed in\", \"1896\"],\n",
      "       [\"Chimnabai Clock Tower\", \"is situated in\", \"Raopura area of Vadodara, Gujarat, India\"],\n",
      "       [\"Chimnabai Clock Tower\", \"was named in memory of\", \"Chimnabai I\"],\n",
      "       [\"Chimnabai I\", \"was\", \"queen\"],\n",
      "       [\"Chimnabai I\", \"was first wife of\", \"Sayajirao Gaekwad III\"],\n",
      "       [\"Sayajirao Gaekwad III\", \"was from\", \"Baroda State\"],\n",
      "       [\"PSLV-C56 mission\", \"aims to launch\", \"satellite into orbit\"],\n",
      "       [\"PSLV-C56 mission\", \"aims to study\", \"weather patterns\"],\n",
      "       [\"James Cameron\", \"did not direct\", \"film Oppenheimer\"],\n",
      "       [\"Tom Cruise\", \"does not star as\", \"J. Robert Oppenheimer\"],\n",
      "       [\"Chimnabai Clock Tower\", \"was built in\", \"Indo-Saracenic architecture style\"],\n",
      "       [\"Mir Kamaluddin Hussainkhan\", \"inaugurated\", \"Chimnabai Clock Tower\"],\n",
      "       [\"Chimnabai Clock Tower\", \"was a stoppage for\", \"horse drawn trams\"],\n",
      "       [\"Chimnabai Clock Tower\", \"was built at the cost of\", \"25,000\"]\n",
      "   ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting to Neo4j: 100%|██████████| 18/18 [00:26<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge Graph constructed in Neo4j container.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in tqdm(semantic_chunks, desc=\"Ingesting to Neo4j\"):\n",
    "    triplets = extract_triplets(chunk)\n",
    "    print(triplets)\n",
    "    parsed = json.loads(triplets)\n",
    "    triplets = parsed.get(\"triplets\", [])\n",
    "    save_to_neo4j(triplets)\n",
    "\n",
    "print(\"\\nKnowledge Graph constructed in Neo4j container.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2b073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
