{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8648517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from neo4j_graphrag.llm.base import LLMInterface\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from dotenv import load_dotenv \n",
    "from typing import Any, Optional\n",
    "from neo4j_graphrag.llm.types import LLMResponse\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings \n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120bc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from neo4j_graphrag.experimental.components.text_splitters.langchain import LangChainTextSplitterAdapter\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"), \n",
    "    breakpoint_threshold_type=\"percentile\" \n",
    ")\n",
    "\n",
    "neo4j_semantic_splitter = LangChainTextSplitterAdapter(semantic_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "class GroqAdapter(LLMInterface):\n",
    "    def __init__(self, llm: Any):\n",
    "        self.llm = llm\n",
    "\n",
    "    def invoke(self, input_text: str, *args: Any, **kwargs: Any) -> LLMResponse:\n",
    "        print(\"input_text: \", input_text)\n",
    "        print(\"args: \",args)\n",
    "        print(\"kwargs: \",kwargs)\n",
    "        system_instruction = kwargs.get(\"system_instruction\", \"\")\n",
    "        \n",
    "        if \"json\" not in system_instruction.lower():\n",
    "            system_instruction += \" Please provide the output in valid JSON format.\"\n",
    "\n",
    "        if system_instruction:\n",
    "            messages = [\n",
    "                (\"system\", system_instruction),\n",
    "                (\"human\", input_text)\n",
    "            ]\n",
    "            response = self.llm.invoke(messages)\n",
    "        else:\n",
    "            response = self.llm.invoke(input_text + \" (Output in JSON)\")\n",
    "            \n",
    "        return LLMResponse(content=response.content)\n",
    "\n",
    "    async def ainvoke(self, input_text: str, *args: Any, **kwargs: Any) -> LLMResponse:\n",
    "        system_instruction = kwargs.get(\"system_instruction\")\n",
    "        print(\"input_text: \", input_text)\n",
    "        print(\"args: \",args)\n",
    "        print(\"kwargs: \",kwargs)\n",
    "        if system_instruction:\n",
    "            messages = [(\"system\", system_instruction), (\"human\", input_text)]\n",
    "            response = await self.llm.ainvoke(messages)\n",
    "        else:\n",
    "            response = await self.llm.ainvoke(input_text)\n",
    "        return LLMResponse(content=response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d01fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loader = DirectoryLoader(path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "#docs = loader.load()\n",
    "#print(f\"Loaded {len(docs)} document pages.\")\n",
    "path = 'C:/Users/Cengizhan/Desktop/CMPE492-Project-Rag-Pipeline/Documents/Ragas/ragas_2309.15217v2.pdf'\n",
    "URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "AUTH = (os.getenv(\"NEO4J_USERNAME\", \"neo4j\"), os.getenv(\"NEO4J_PASSWORD\", \"password\"))\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") \n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "groq_llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", \n",
    "    temperature=0,\n",
    "    api_key=GROQ_API_KEY ,\n",
    "    max_tokens=4096,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}} \n",
    ")\n",
    "neo4j_embedder = SentenceTransformerEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm_adapter = GroqAdapter(llm=groq_llm)\n",
    "\n",
    "kg_pipeline = SimpleKGPipeline(\n",
    "    llm=llm_adapter,\n",
    "    driver=driver,\n",
    "    from_pdf=True,\n",
    "    embedder= neo4j_embedder,\n",
    "    schema=\"FREE\",\n",
    "    text_splitter=neo4j_semantic_splitter,\n",
    "    on_error=\"IGNORE\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cf272",
   "metadata": {},
   "outputs": [],
   "source": [
    "await kg_pipeline.run_async(file_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KG Pipeline Tracker:\n",
    "## run_async() take the content of the page  and  call runner on it\n",
    "## FixedSizeSplitter to split the page into chunks (character count)\n",
    "## TextChunkEmbedder to create embedding of each chunk\n",
    "## SchemaFromTextExtractor to create schema with LLM\n",
    "### prompt for this :\"\"You are a top-tier algorithm designed for extracting a labeled property graph schema in structured formats......\n",
    "##LLMEntityRelationExtractor : extract the relationships and nodes from the text, it uses the only types and labels from the graph schema that SchemaFromTextExtractor provides\n",
    "## GraphPruning to compare every extracted node label and relationship type against the GraphSchema\n",
    "### It identifies relationships where the start_node or end_node is missing from the list of extracted nodes.\n",
    "## KGWriter : write the nodes and relationships to the neo4j\n",
    "##SinglePropertyExactMatchResolver : Resolve entities with same label and exact same property (default is \"name\").\n",
    "### LLM might create with different id but same properties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b5e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "create_vector_index(\n",
    "    driver, \n",
    "    name=\"chunk_vector\", \n",
    "    label=\"Chunk\", \n",
    "    embedding_property=\"embedding\", \n",
    "    dimensions=384,  \n",
    "    similarity_fn=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bfbd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"chunk_vector\",\n",
    "    embedder=neo4j_embedder,\n",
    "    retrieval_query=\"\"\"\n",
    "    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\n",
    "    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\n",
    "    RETURN node.text AS text, \n",
    "           node.score AS score, \n",
    "           {related_info: info} AS metadata\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1cdafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':NEXT_CHUNK|FROM_CHUNK' instead)} {position: line: 2, column: 30, offset: 179} for query: 'CALL db.index.vector.queryNodes($vector_index_name, $top_k * $effective_search_ratio, $query_vector) YIELD node, score WITH node, score LIMIT $top_k \\n    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\\n    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\\n    RETURN node.text AS text, \\n           node.score AS score, \\n           {related_info: info} AS metadata\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: score)} {position: line: 5, column: 17, offset: 343} for query: 'CALL db.index.vector.queryNodes($vector_index_name, $top_k * $effective_search_ratio, $query_vector) YIELD node, score WITH node, score LIMIT $top_k \\n    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\\n    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\\n    RETURN node.text AS text, \\n           node.score AS score, \\n           {related_info: info} AS metadata\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Context:\n",
      "<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.' score=None metadata={'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', 'Luis Espinosa-Anke', 'GPT-4', '2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.', 'AMPLYFI', 'Steven Schockaert', 'Exploding Gradients', 'Shahul Es']}>\n",
      "<Record text='2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.' score=None metadata={'related_info': ['Ece Kamar', 'Sébastien Bubeck', 'Ming-Wei Chang', '__Entity__', 'See-Kiong Ng', '__Entity__', 'Peter Lee', 'Zhengbao Jiang', 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics', 'Johannes Gehrke', '2023. Gptscore: Evaluate as you desire. CoRR,\\nabs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang.', 'Yuanzhi Li', 'Jinlan Fu', 'Scott Lundberg', 'Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.', 'Kenton Lee', 'Kristina Toutanova', 'Pengfei Liu', 'Eric Horvitz', 'International Conference on Machine Learning', '__Entity__', 'Varun Chandrasekaran', 'Jacob Devlin', 'Yin Tat Lee', 'Ronen Eldan']}>\n",
      "<Record text='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.' score=None metadata={'related_info': ['__Entity__', 'Dor Muhlgay', 'CoRR', 'Ori Ram', 'Yoav Levine', 'Yoav Shoham', 'Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.', '__Entity__', 'Kevin Leyton-Brown', 'Itay Dalmedigos', 'Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.', 'Amnon Shashua']}>\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Question:\n",
      "What are the core components of the RAG pipeline mentioned in the document?\n",
      "\n",
      "Answer:\n",
      "\n",
      "retrieve result {'items': [{'content': \"<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\\\n†Exploding Gradients\\\\n∗CardiffNLP, Cardiff Universi... (23235 chars)\", 'metadata': {'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', '... (7 items)']}}, {'content': \"<Record text='2022. Improving language models by retrieving from\\\\ntrillions of tokens. In International Conference on\\\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\\\ntimore, Maryland, USA, volume... (23706 chars)\", 'metadata': {'related_info': ['Ece Kamar', 'Sébastien Bubeck', 'Ming-Wei Chang', '__Entity__', 'See-Kiong Ng', '... (20 items)']}}, {'content': \"<Record text='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\\\nShoham. 2023. In-context retrieval-augmented lan-\\\\nguage models. CoRR, abs/2302.00083.' ... (955 chars)\", 'metadata': {'related_info': ['__Entity__', 'Dor Muhlgay', 'CoRR', 'Ori Ram', 'Yoav Levine', '... (7 items)']}}], 'metadata': {'query_vector': [-0.10571019351482391, 0.07304587215185165, 0.04226483777165413, -0.007508380804210901, -0.050810493528842926, '... (379 items)'], '__retriever': 'VectorCypherRetriever'}}\n",
      "input_text:  Context:\n",
      "<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.' score=None metadata={'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', 'Luis Espinosa-Anke', 'GPT-4', '2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.', 'AMPLYFI', 'Steven Schockaert', 'Exploding Gradients', 'Shahul Es']}>\n",
      "<Record text='2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.' score=None metadata={'related_info': ['Ece Kamar', 'Sébastien Bubeck', 'Ming-Wei Chang', '__Entity__', 'See-Kiong Ng', '__Entity__', 'Peter Lee', 'Zhengbao Jiang', 'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics', 'Johannes Gehrke', '2023. Gptscore: Evaluate as you desire. CoRR,\\nabs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang.', 'Yuanzhi Li', 'Jinlan Fu', 'Scott Lundberg', 'Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.', 'Kenton Lee', 'Kristina Toutanova', 'Pengfei Liu', 'Eric Horvitz', 'International Conference on Machine Learning', '__Entity__', 'Varun Chandrasekaran', 'Jacob Devlin', 'Yin Tat Lee', 'Ronen Eldan']}>\n",
      "<Record text='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.' score=None metadata={'related_info': ['__Entity__', 'Dor Muhlgay', 'CoRR', 'Ori Ram', 'Yoav Levine', 'Yoav Shoham', 'Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.', '__Entity__', 'Kevin Leyton-Brown', 'Itay Dalmedigos', 'Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.', 'Amnon Shashua']}>\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Question:\n",
      "What are the core components of the RAG pipeline mentioned in the document?\n",
      "\n",
      "Answer:\n",
      "\n",
      "args:  (None,)\n",
      "kwargs:  {'system_instruction': 'Answer the user question using the provided context.'}\n",
      "llm_response content='{\\n\"retrieval\": \"The retrieval module is one of the core components of the RAG pipeline, responsible for retrieving relevant context passages from a corpus.\",\\n  \"LLM-based generation module\": \"The LLM-based generation module is another core component of the RAG pipeline, which generates an answer based on the retrieved context and the original question.\",\\n  \"knowledge database\": \"A reference textual database that provides knowledge to the LLMs, enabling them to act as a natural language layer between a user and textual databases.\"\\n}'\n",
      "Answer: {\n",
      "\"retrieval\": \"The retrieval module is one of the core components of the RAG pipeline, responsible for retrieving relevant context passages from a corpus.\",\n",
      "  \"LLM-based generation module\": \"The LLM-based generation module is another core component of the RAG pipeline, which generates an answer based on the retrieved context and the original question.\",\n",
      "  \"knowledge database\": \"A reference textual database that provides knowledge to the LLMs, enabling them to act as a natural language layer between a user and textual databases.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rag_system = GraphRAG(\n",
    "    retriever=retriever, \n",
    "    llm=llm_adapter       \n",
    ")\n",
    "\n",
    "query = \"What are the core components of the RAG pipeline mentioned in the document?\"\n",
    "response = rag_system.search(\n",
    "    query_text=query, \n",
    "    retriever_config={\"top_k\": 3}\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d5e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
