{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8648517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from neo4j_graphrag.llm.base import LLMInterface\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from dotenv import load_dotenv \n",
    "from typing import Any, Optional\n",
    "from neo4j_graphrag.llm.types import LLMResponse\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings \n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120bc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from neo4j_graphrag.experimental.components.text_splitters.langchain import LangChainTextSplitterAdapter\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"), \n",
    "    breakpoint_threshold_type=\"percentile\" \n",
    ")\n",
    "\n",
    "neo4j_semantic_splitter = LangChainTextSplitterAdapter(semantic_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5828e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "class GroqAdapter(LLMInterface):\n",
    "    def __init__(self, llm: Any):\n",
    "        self.llm = llm\n",
    "\n",
    "    def invoke(self, input_text: str, *args: Any, **kwargs: Any) -> LLMResponse:\n",
    "        print(\"input_text: \", input_text)\n",
    "        print(\"args: \",args)\n",
    "        print(\"kwargs: \",kwargs)\n",
    "        system_instruction = kwargs.get(\"system_instruction\", \"\")\n",
    "        \n",
    "        if \"json\" not in system_instruction.lower():\n",
    "            system_instruction += \" Please provide the output in valid JSON format.\"\n",
    "\n",
    "        if system_instruction:\n",
    "            messages = [\n",
    "                (\"system\", system_instruction),\n",
    "                (\"human\", input_text)\n",
    "            ]\n",
    "            response = self.llm.invoke(messages)\n",
    "        else:\n",
    "            response = self.llm.invoke(input_text + \" (Output in JSON)\")\n",
    "            \n",
    "        return LLMResponse(content=response.content)\n",
    "\n",
    "    async def ainvoke(self, input_text: str, *args: Any, **kwargs: Any) -> LLMResponse:\n",
    "        system_instruction = kwargs.get(\"system_instruction\")\n",
    "        print(\"input_text: \", input_text)\n",
    "        print(\"args: \",args)\n",
    "        print(\"kwargs: \",kwargs)\n",
    "        if system_instruction:\n",
    "            messages = [(\"system\", system_instruction), (\"human\", input_text)]\n",
    "            response = await self.llm.ainvoke(messages)\n",
    "        else:\n",
    "            response = await self.llm.ainvoke(input_text)\n",
    "        return LLMResponse(content=response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d01fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loader = DirectoryLoader(path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "#docs = loader.load()\n",
    "#print(f\"Loaded {len(docs)} document pages.\")\n",
    "path = 'C:/Users/Cengizhan/Desktop/CMPE492-Project-Rag-Pipeline/Documents/Ragas/ragas_2309.15217v2.pdf'\n",
    "URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "AUTH = (os.getenv(\"NEO4J_USERNAME\", \"neo4j\"), os.getenv(\"NEO4J_PASSWORD\", \"password\"))\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") \n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "groq_llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", \n",
    "    temperature=0,\n",
    "    api_key=GROQ_API_KEY ,\n",
    "    max_tokens=4096,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}} \n",
    ")\n",
    "neo4j_embedder = SentenceTransformerEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm_adapter = GroqAdapter(llm=groq_llm)\n",
    "\n",
    "kg_pipeline = SimpleKGPipeline(\n",
    "    llm=llm_adapter,\n",
    "    driver=driver,\n",
    "    from_pdf=True,\n",
    "    embedder= neo4j_embedder,\n",
    "    schema=\"FREE\",\n",
    "    text_splitter=neo4j_semantic_splitter,\n",
    "    on_error=\"IGNORE\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64cf272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Ragas: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\n",
      "†Exploding Gradients\n",
      "∗CardiffNLP, Cardiff University, United Kingdom\n",
      "♢AMPLYFI, United Kingdom\n",
      "shahules786@gmail.com,jamesjithin97@gmail.com\n",
      "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "Abstract\n",
      "We introduce Ragas (Retrieval Augmented\n",
      "Generation Assessment), a framework for\n",
      "reference-free evaluation of Retrieval Aug-\n",
      "mented Generation (RAG) pipelines. RAG\n",
      "systems are composed of a retrieval and an\n",
      "LLM based generation module, and provide\n",
      "LLMs with knowledge from a reference textual\n",
      "database, which enables them to act as a natu-\n",
      "ral language layer between a user and textual\n",
      "databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\n",
      "lenging because there are several dimensions to\n",
      "consider: the ability of the retrieval system to\n",
      "identify relevant and focused context passages,\n",
      "the ability of the LLM to exploit such passages\n",
      "in a faithful way, or the quality of the genera-\n",
      "tion itself. With Ragas, we put forward a suite\n",
      "of metrics which can be used to evaluate these\n",
      "different dimensions without having to rely on\n",
      "ground truth human annotations. We posit that\n",
      "such a framework can crucially contribute to\n",
      "faster evaluation cycles of RAG architectures,\n",
      "which is especially important given the fast\n",
      "adoption of LLMs. 1 Introduction\n",
      "Language Models (LMs) capture a vast amount\n",
      "of knowledge about the world, which allows them\n",
      "to answer questions without accessing any exter-\n",
      "nal sources. This idea of LMs as repositories of\n",
      "knowledge emerged shortly after the introduction\n",
      "of BERT (Devlin et al., 2019) and became more\n",
      "firmly established with the introduction of ever\n",
      "larger LMs (Roberts et al., 2020). While the most\n",
      "recent Large Language Models (LLMs) capture\n",
      "enough knowledge to rival human performance\n",
      "across a wide variety of question answering bench-\n",
      "marks (Bubeck et al., 2023), the idea of using\n",
      "LLMs as knowledge bases still has two fundamen-\n",
      "tal limitations. First, LLMs are not able to answer\n",
      "questions about events that have happened after\n",
      "they were trained. Second, even the largest models\n",
      "struggle to memorise knowledge that is only rarely\n",
      "mentioned in the training corpus (Kandpal et al.,\n",
      "2022; Mallen et al., 2023). The standard solution\n",
      "to these issues is to rely on Retrieval Augmented\n",
      "Generation (RAG) (Lee et al., 2019; Lewis et al.,\n",
      "2020; Guu et al., 2020). Answering a question\n",
      "then essentially involves retrieving relevant pas-\n",
      "sages from a corpus and feeding these passages,\n",
      "along with the original question, to the LM. While\n",
      "initial approaches relied on specialised LMs for\n",
      "retrieval-augmented language modelling (Khandel-\n",
      "wal et al., 2020; Borgeaud et al., 2022), recent work\n",
      "has suggested that simply adding retrieved docu-\n",
      "ments to the input of a standard LM can also work\n",
      "well (Khattab et al., 2022; Ram et al., 2023; Shi\n",
      "et al., 2023), thus making it possible to use retrieval-\n",
      "augmented strategies in combination with LLMs\n",
      "that are only available through APIs. While the usefulness of retrieval-augmented\n",
      "strategies is clear, their implementation requires\n",
      "a significant amount of tuning, as the overall per-\n",
      "formance will be affected by the retrieval model,\n",
      "the considered corpus, the LM, or the prompt for-\n",
      "mulation, among others. Automated evaluation of\n",
      "retrieval-augmented systems is thus paramount. In\n",
      "practice, RAG systems are often evaluated in terms\n",
      "of the language modelling task itself, i.e. by mea-\n",
      "suring perplexity on some reference corpus. How-\n",
      "ever, such evaluations are not always predictive\n",
      "of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\n",
      "probabilities, which are not accessible for some\n",
      "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
      "tion answering is another common evaluation task,\n",
      "but usually only datasets with short extractive an-\n",
      "swers are considered, which may not be represen-\n",
      "tative of how the system will be used. To address these issues, in this paper we present\n",
      "Ragas1, a framework for the automated assessment\n",
      "1Ragas is available at https://github.com/\n",
      "explodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\n",
      "of retrieval augmented generation systems. We fo-\n",
      "cus on settings where reference answers may not be\n",
      "available, and where we want to estimate different\n",
      "proxies for correctness, in addition to the useful-\n",
      "ness of the retrieved passages. The Ragas frame-\n",
      "work provides an integration with both llama-index\n",
      "and Langchain, the most widely used frameworks\n",
      "for building RAG solutions, thus enabling devel-\n",
      "opers to easily integrate Ragas into their standard\n",
      "workflow. 2 Related Work\n",
      "Estimating faithfulness using LLMs The prob-\n",
      "lem of detecting hallucinations in LLM generated\n",
      "responses has been extensively studied (Ji et al.,\n",
      "2023). Several authors have suggested the idea\n",
      "of predicting factuality using a few-shot prompt-\n",
      "ing strategy (Zhang et al., 2023). Recent analy-\n",
      "ses, however, suggest that existing models struggle\n",
      "with detecting hallucination when using standard\n",
      "prompting strategies (Li et al., 2023; Azaria and\n",
      "Mitchell, 2023). Other approaches rely on linking\n",
      "the generated responses to facts from an external\n",
      "knowledge base (Min et al., 2023), but this is not\n",
      "always possible. Yet another strategy is to inspect the probabili-\n",
      "ties assigned to individual tokens, where we would\n",
      "expect the model to be less confident in halluci-\n",
      "nated answers than in factual ones. For instance,\n",
      "BARTScore (Yuan et al., 2021) estimates factuality\n",
      "by looking at the conditional probability of the gen-\n",
      "erated text given the input. Kadavath et al. (2022)\n",
      "use a variation of this idea. Starting from the ob-\n",
      "servation that LLMs provide well-calibrated proba-\n",
      "bilities when answering multiple-choice questions,\n",
      "they essentially convert the problem of validating\n",
      "model generated answers into a multiple-choice\n",
      "question which asks whether the answer is true or\n",
      "false. Rather than looking at the output probabil-\n",
      "ities, Azaria and Mitchell (2023) propose to train\n",
      "a supervised classifier on the weights from one of\n",
      "the hidden layers of the LLM, to predict whether a\n",
      "given statement is true or not. While the approach\n",
      "performs well, the need to access the hidden states\n",
      "of the model makes it unsuitable for systems that\n",
      "access LLMs through an API. For models that do not provide access to token\n",
      "probabilities, such as ChatGPT and GPT-4, differ-\n",
      "ent methods are needed. SelfCheckGPT (Manakul\n",
      "et al., 2023) addresses this problem by instead sam-\n",
      "pling multiple answers. Their core idea is that\n",
      "factual answers are more stable: when an answer is\n",
      "factual, we can expect that different samples will\n",
      "tend to be semantically similar, whereas this is less\n",
      "likely to be the case for hallucinated answers. Automated evaluation of text generation systems\n",
      "LLMs have also been leveraged to automatically\n",
      "evaluate other aspects of generated text fragments,\n",
      "beyond factuality. For instance, GPTScore (Fu\n",
      "et al., 2023) uses a prompt that specifies the consid-\n",
      "ered aspect (e.g. fluency) and then scores passages\n",
      "based on the average probability of the generated\n",
      "tokens, according to a given autoregressive LM. This idea of using prompts was previously also\n",
      "considered by Yuan et al. (2021), although they\n",
      "used a smaller fine-tuned LM (i.e. BART) and did\n",
      "not observe a clear benefit from using prompts. An-\n",
      "other approach directly asks ChatGPT to evaluate\n",
      "a particular aspect of the given answer by provid-\n",
      "ing a score between 0 and 100, or by providing a\n",
      "rating on a 5-star scale (Wang et al., 2023a). Re-\n",
      "markably, strong results can be obtained in this\n",
      "way, although it comes with the limitation of being\n",
      "sensitive to the design of the prompt. Rather than\n",
      "scoring individual answers, some authors have also\n",
      "focused on using an LLM to select the best answer\n",
      "among a number of candidates (Wang et al., 2023b),\n",
      "typically to compare the performance of different\n",
      "LLMs. However, care is needed with this approach,\n",
      "as the order in which the answers is presented can\n",
      "influence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\n",
      "generally, generations, have been typically used\n",
      "in the literature, most approaches have relied on\n",
      "the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\n",
      "and MoverScore (Zhao et al., 2019) use contex-\n",
      "tualised embeddings, produced by a pre-trained\n",
      "BERT model, to compare the similarity between\n",
      "the generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\n",
      "ence answers to compute aspects such as precision\n",
      "(estimated as the probability of generating the gen-\n",
      "erated answer given the reference) and recall (esti-\n",
      "mated as the probability of generating the reference\n",
      "given the generated answer). 3 Evaluation Strategies\n",
      "We consider a standard RAG setting, where given a\n",
      "question q, the system first retrieves some context\n",
      "c(q) and then uses the retrieved context to generate\n",
      "an answer as(q). When building a RAG system,\n",
      "we usually do not have access to human-annotated\n",
      "datasets or reference answers. We therefore fo-\n",
      "cus on metrics that are fully self-contained and\n",
      "reference-free. We focus in particular three quality\n",
      "aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\n",
      "swer should be grounded in the given context. This\n",
      "is important to avoid hallucinations, and to ensure\n",
      "that the retrieved context can act as a justification\n",
      "for the generated answer. Indeed, RAG systems are\n",
      "often used in applications where the factual con-\n",
      "sistency of the generated text w.r.t. the grounded\n",
      "sources is highly important, e.g. in domains such as\n",
      "law, where information is constantly evolving. Sec-\n",
      "ond, Answer Relevance refers to the idea that the\n",
      "generated answer should address the actual ques-\n",
      "tion that was provided. Finally,Context Relevance\n",
      "refers to the idea that the retrieved context should\n",
      "be focused, containing as little irrelevant informa-\n",
      "tion as possible. This is important given the cost\n",
      "associated with feeding long context passages to\n",
      "LLMs. Moreover, when context passages are too\n",
      "long, LLMs are often less effective in exploiting\n",
      "that context, especially for information that is pro-\n",
      "vided in the middle of the context passage (Liu\n",
      "et al., 2023). We now explain how these three quality aspects\n",
      "can be measured in a fully automated way, by\n",
      "prompting an LLM. In our implementation and\n",
      "experiments, all prompts are evaluated using the\n",
      "gpt-3.5-turbo-16k model, which is available\n",
      "through the OpenAI API2. Faithfulness We say that the answer as(q) is\n",
      "faithful to the context c(q) if the claims that are\n",
      "made in the answer can be inferred from the con-\n",
      "text. To estimate faithfulness, we first use an LLM\n",
      "to extract a set of statements, S(as(q)). The aim\n",
      "of this step is to decompose longer sentences into\n",
      "shorter and more focused assertions. We use the\n",
      "following prompt for this step3:\n",
      "Given a question and answer, create one\n",
      "or more statements from each sentence\n",
      "in the given answer. question: [question]\n",
      "answer: [answer]\n",
      "where [question] and [answer] refer to the\n",
      "given question and answer. For each statement si\n",
      "2https://platform.openai.com\n",
      "3To help clarify the task, we include a demonstration as\n",
      "part of the prompt. This demonstration is not explicitly shown\n",
      "in the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\n",
      "c(q) using a verification function v(si, c(q)). This\n",
      "verification step is carried out using the following\n",
      "prompt:\n",
      "Consider the given context and following\n",
      "statements, then determine whether they\n",
      "are supported by the information present\n",
      "in the context. Provide a brief explana-\n",
      "tion for each statement before arriving\n",
      "at the verdict (Yes/No). Provide a final\n",
      "verdict for each statement in order at the\n",
      "end in the given format. Do not deviate\n",
      "from the specified format. statement: [statement 1]\n",
      "... statement: [statement n]\n",
      "The final faithfulness score, F , is then computed\n",
      "as F = |V |\n",
      "|S| , where |V | is the number of statements\n",
      "that were supported according to the LLM and |S|\n",
      "is the total number of statements. Answer relevance We say that the answer as(q)\n",
      "is relevant if it directly addresses the question in\n",
      "an appropriate way. In particular, our assessment\n",
      "of answer relevance does not take into account fac-\n",
      "tuality, but penalises cases where the answer is\n",
      "incomplete or where it contains redundant informa-\n",
      "tion. To estimate answer relevance, for the given\n",
      "answer as(q), we prompt the LLM to generate n\n",
      "potential questions qi based on as(q), as follows:\n",
      "Generate a question for the given answer. answer: [answer]\n",
      "We then obtain embeddings for all questions us-\n",
      "ing the text-embedding-ada-002 model, avail-\n",
      "able from the OpenAI API. For each qi, we cal-\n",
      "culate the similarity sim(q, qi) with the original\n",
      "question q, as the cosine between the correspond-\n",
      "ing embeddings. The answer relevance score, AR,\n",
      "for question q is then computed as:\n",
      "AR = 1\n",
      "n\n",
      "nX\n",
      "i=1\n",
      "sim(q, qi) (1)\n",
      "This metric evaluates how closely the generated\n",
      "answer aligns with the initial question or instruc-\n",
      "tion. Context relevance The context c(q) is consid-\n",
      "ered relevant to the extent that it exclusively con-\n",
      "tains information that is needed to answer the ques-\n",
      "tion. In particular, this metric aims to penalise the\n",
      "inclusion of redundant information. To estimate\n",
      "context relevance, given a question q and its con-\n",
      "text c(q), the LLM extracts a subset of sentences,\n",
      "Sext, from c(q) that are crucial to answer q, using\n",
      "the following prompt:\n",
      "Please extract relevant sentences from\n",
      "the provided context that can potentially\n",
      "help answer the following question. If no\n",
      "relevant sentences are found, or if you\n",
      "believe the question cannot be answered\n",
      "from the given context, return the phrase\n",
      "\"Insufficient Information\". While extract-\n",
      "ing candidate sentences you’re not al-\n",
      "lowed to make any changes to sentences\n",
      "from given context. The context relevance score is then computed as:\n",
      "CR = number of extracted sentences\n",
      "total number of sentences in c(q) (2)\n",
      "4 The WikiEval Dataset\n",
      "To evaluate the proposed framework, we ideally\n",
      "need examples of question-context-answer triples\n",
      "which are annotated with human judgments. We\n",
      "can then verify to what extent our metrics agree\n",
      "with human assessments of faithfulness, answer\n",
      "relevance and context relevance. Since we are not\n",
      "aware of any publicly available datasets that could\n",
      "be used for this purpose, we created a new dataset,\n",
      "which we refer to as WikiEval4. To construct the\n",
      "dataset, we first selected 50 Wikipedia pages cov-\n",
      "ering events that have happened since the start of\n",
      "20225. In selecting these pages, we prioritised\n",
      "those with recent edits. For each of the 50 pages,\n",
      "we then asked ChatGPT to suggest a question that\n",
      "can be answered based on the introductory section\n",
      "of the page, using the following prompt:\n",
      "Your task is to formulate a question from\n",
      "given context satisfying the rules given\n",
      "below:\n",
      "1. The question should be fully answered\n",
      "from the given context. 2. The question should be framed from\n",
      "a part that contains non-trivial informa-\n",
      "tion. 3. The answer should not contain any\n",
      "4https://huggingface.co/datasets/\n",
      "explodinggradients/WikiEval\n",
      "5That is, beyond the reported training cutoff of the model\n",
      "we used in our experiments. links. 4. The question should be of moderate\n",
      "difficulty. 5. The question must be reasonable and\n",
      "must be understood and responded to by\n",
      "humans. 6. Do not use phrases that ’provided con-\n",
      "text’, etc in the question\n",
      "context:\n",
      "We also used ChatGPT to answer the generated\n",
      "question, when given the corresponding introduc-\n",
      "tory section as context, using the following prompt:\n",
      "Answer the question using the informa-\n",
      "tion from the given context. question: [question]\n",
      "context: [context]\n",
      "All questions were annotated along the three con-\n",
      "sidered quality dimensions by two annotators. Both\n",
      "annotators were fluent in English and were given\n",
      "clear instructions about the meaning of the three\n",
      "considered quality dimensions. For faithfulness\n",
      "and context relevance, the two annotators agreed in\n",
      "around 95% of cases. For answer relevance, they\n",
      "agreed in around 90% of the cases. Disagreements\n",
      "were resolved after a discussion between the anno-\n",
      "tators. Faithfulness To obtain human judgements about\n",
      "faithfulness, we first used ChatGPT to answer the\n",
      "question without access to any additional context. We then asked the annotators to judge which of the\n",
      "two answers was the most faithful (i.e. the standard\n",
      "one or the one generated without context), given\n",
      "the question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\n",
      "obtain candidate answers with lower answer rel-\n",
      "evance, using the following prompt:\n",
      "Answer the given question in an incom-\n",
      "plete manner. question: [question]\n",
      "We then asked human annotators to compare this\n",
      "answer, and indicate which of the two answers had\n",
      "the highest answer relevance. Context relevance To measure this aspect, we\n",
      "first added additional sentences to the context by\n",
      "scraping back-links to the corresponding Wikipedia\n",
      "page. In this way, we were able to add information\n",
      "to the context that was related but less relevant for\n",
      "Faith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\n",
      "GPT Score 0.72 0.52 0.63\n",
      "GPT Ranking 0.54 0.40 0.52\n",
      "Table 1: Agreement with human annotators in pairwise\n",
      "comparisons of faithfulness, answer relevance and con-\n",
      "text relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\n",
      "out any back-links, we instead used ChatGPT to\n",
      "complete the given context. 5 Experiments\n",
      "Table 1 analyses the agreement between the met-\n",
      "rics proposed in Section 3 and the human assess-\n",
      "ments from the proposed WikiEval dataset. Each\n",
      "WikiEval instance requires the model to compare\n",
      "two answers or two context fragments. We count\n",
      "how often the answer/context preferred by the\n",
      "model (i.e. with highest estimated faithfulness, an-\n",
      "swer relevance, or context relevance) coincides\n",
      "with the answer/context preferred by the human\n",
      "annotators. We report the results in terms of ac-\n",
      "curacy (i.e. the fraction of instances on which the\n",
      "model agrees with the annotators). To put the results in context, we compare our\n",
      "proposed metrics (shown as Ragas in Table 1) with\n",
      "two baseline methods. For the first method, shown\n",
      "as GPT Score, we ask ChatGPT to assign a score\n",
      "between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\n",
      "meaning of the quality metric and then asks to\n",
      "score the given answer/context in line with that\n",
      "definition. For instance, for evaluating faithfulness,\n",
      "we used the following prompt:\n",
      "Faithfulness measures the information\n",
      "consistency of the answer against the\n",
      "given context. Any claims that are made\n",
      "in the answer that cannot be deduced\n",
      "from context should be penalized. Given an answer and context, assign a\n",
      "score for faithfulness in the range 0-10. context: [context]\n",
      "answer: [answer]\n",
      "Ties, where the same score is assigned by the LLM\n",
      "to both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\n",
      "stead asks ChatGPT to select the preferred answer/-\n",
      "context. In this case, the prompt again includes\n",
      "a definition of the considered quality metric. For\n",
      "instance, for evaluating answer relevance, we used\n",
      "the following prompt:\n",
      "Answer Relevancy measures the degree\n",
      "to which a response directly addresses\n",
      "and is appropriate for a given question. It penalizes the present of redundant in-\n",
      "formation or incomplete answers given a\n",
      "question. Given an question and answer,\n",
      "rank each answer based on Answer Rele-\n",
      "vancy. question: [question]\n",
      "answer 1: [answer 1]\n",
      "answer 2: [answer 2]\n",
      "The results in Table 1 show that our proposed\n",
      "metrics are much closer aligned with the human\n",
      "judgements than the predictions from the two base-\n",
      "lines. For faithfulness, the Ragas prediction are in\n",
      "general highly accurate. For answer relevance, the\n",
      "agreement is lower, but this is largely due to the\n",
      "fact that the differences between the two candidate\n",
      "answers are often very subtle. We found context\n",
      "relevance to be the hardest quality dimension to\n",
      "evaluate. In particular, we observed that ChatGPT\n",
      "often struggles with the task of selecting the sen-\n",
      "tences from the context that are crucial, especially\n",
      "for longer contexts. 6 Conclusions\n",
      "We have highlighted the need for automated\n",
      "reference-free evaluation of RAG systems. In par-\n",
      "ticular, we have argued the need for an evaluation\n",
      "framework that can assess faithfulness (i.e. is the\n",
      "answer grounded in the retrieved context), answer\n",
      "relevance (i.e. does the answer address the ques-\n",
      "tion) and context relevance (i.e. is the retrieved\n",
      "context sufficiently focused). To support the devel-\n",
      "opment of such a framework, we have introduced\n",
      "WikiEval, a dataset which human judgements of\n",
      "these three different aspects. Finally, we have also\n",
      "described Ragas, our implementation of the three\n",
      "considered quality aspects. This framework is easy\n",
      "to use and can provide deverlopers of RAG sys-\n",
      "tems with valuable insights, even in the absence\n",
      "of any ground truth. Our evaluation on WikiEval\n",
      "has shown that the predictions from Ragas are\n",
      "closely aligned with human predictions, especially\n",
      "for faithfulness and answer relevance. References\n",
      "Amos Azaria and Tom M. Mitchell. 2023. The inter-\n",
      "nal state of an LLM knows when its lying. CoRR,\n",
      "abs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\n",
      "Trevor Cai, Eliza Rutherford, Katie Millican, George\n",
      "van den Driessche, Jean-Baptiste Lespiau, Bogdan\n",
      "Damoc, Aidan Clark, Diego de Las Casas, Aurelia\n",
      "Guy, Jacob Menick, Roman Ring, Tom Hennigan,\n",
      "Saffron Huang, Loren Maggiore, Chris Jones, Albin\n",
      "Cassirer, Andy Brock, Michela Paganini, Geoffrey\n",
      "Irving, Oriol Vinyals, Simon Osindero, Karen Si-\n",
      "monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2022. Improving language models by retrieving from\n",
      "trillions of tokens. In International Conference on\n",
      "Machine Learning, ICML 2022, 17-23 July 2022, Bal-\n",
      "timore, Maryland, USA, volume 162 of Proceedings\n",
      "of Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\n",
      "dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\n",
      "Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\n",
      "berg, et al. 2023. Sparks of artificial general intelli-\n",
      "gence: Early experiments with gpt-4. arXiv preprint\n",
      "arXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers), pages\n",
      "4171–4186, Minneapolis, Minnesota. Association for\n",
      "Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\n",
      "Liu.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2023. Gptscore: Evaluate as you desire. CoRR,\n",
      "abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Mingwei Chang.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2020. Retrieval augmented\n",
      "language model pre-training. In International confer-\n",
      "ence on machine learning, pages 3929–3938. PMLR. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\n",
      "Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\n",
      "Madotto, and Pascale Fung. 2023. Survey of halluci-\n",
      "nation in natural language generation. ACM Comput-\n",
      "ing Surveys, 55(12):1–38. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom\n",
      "Henighan, Dawn Drain, Ethan Perez, Nicholas\n",
      "Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\n",
      "Tran-Johnson, Scott Johnston, Sheer El Showk, Andy\n",
      "Jones, Nelson Elhage, Tristan Hume, Anna Chen,\n",
      "Yuntao Bai, Sam Bowman, Stanislav Fort, Deep\n",
      "Ganguli, Danny Hernandez, Josh Jacobson, Jack-\n",
      "son Kernion, Shauna Kravec, Liane Lovitt, Ka-\n",
      "mal Ndousse, Catherine Olsson, Sam Ringer, Dario\n",
      "Amodei, Tom Brown, Jack Clark, Nicholas Joseph,\n",
      "Ben Mann, Sam McCandlish, Chris Olah, and Jared\n",
      "Kaplan. 2022. Language models (mostly) know what\n",
      "they know. CoRR, abs/2207.05221. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric\n",
      "Wallace, and Colin Raffel. 2022. Large language\n",
      "models struggle to learn long-tail knowledge. CoRR,\n",
      "abs/2211.08411. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2020. Generalization\n",
      "through memorization: Nearest neighbor language\n",
      "models. In 8th International Conference on Learning\n",
      "Representations, ICLR 2020, Addis Ababa, Ethiopia,\n",
      "April 26-30, 2020. OpenReview.net. Omar Khattab, Keshav Santhanam, Xiang Lisa Li,\n",
      "David Hall, Percy Liang, Christopher Potts, and\n",
      "Matei Zaharia. 2022. Demonstrate-search-predict:\n",
      "Composing retrieval and language models for\n",
      "knowledge-intensive NLP. CoRR, abs/2212.14024.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2019. Latent retrieval for weakly supervised open do-\n",
      "main question answering. In Proceedings of the 57th\n",
      "Annual Meeting of the Association for Computational\n",
      "Linguistics, pages 6086–6096. Patrick S.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "H. Lewis, Ethan Perez, Aleksandra Pik-\n",
      "tus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\n",
      "Tim Rocktäschel, Sebastian Riedel, and Douwe\n",
      "Kiela. 2020. Retrieval-augmented generation for\n",
      "knowledge-intensive NLP tasks. In Advances in Neu-\n",
      "ral Information Processing Systems 33: Annual Con-\n",
      "ference on Neural Information Processing Systems\n",
      "2020, NeurIPS 2020, December 6-12, 2020, virtual. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\n",
      "Nie, and Ji-Rong Wen. 2023. Halueval: A large-\n",
      "scale hallucination evaluation benchmark for large\n",
      "language models. CoRR, abs/2305.11747.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\n",
      "jape, Michele Bevilacqua, Fabio Petroni, and Percy\n",
      "Liang. 2023. Lost in the middle: How language\n",
      "models use long contexts. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\n",
      "Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating\n",
      "effectiveness of parametric and non-parametric mem-\n",
      "ories. In Proceedings of the 61st Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers) , pages 9802–9822, Toronto,\n",
      "Canada. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2023. Selfcheckgpt: Zero-resource black-box hal-\n",
      "lucination detection for generative large language\n",
      "models. CoRR, abs/2303.08896.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\n",
      "Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\n",
      "Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac-\n",
      "tual precision in long form text generation. CoRR,\n",
      "abs/2305.14251.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and Yoav\n",
      "Shoham. 2023. In-context retrieval-augmented lan-\n",
      "guage models. CoRR, abs/2302.00083.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param-\n",
      "eters of a language model? In Proceedings of the\n",
      "2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP), pages 5418–5426,\n",
      "Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\n",
      "Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\n",
      "Wen-tau Yih. 2023. REPLUG: retrieval-augmented\n",
      "black-box language models. CoRR, abs/2301.12652.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\n",
      "ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\n",
      "Zhou. 2023a. Is chatgpt a good NLG evaluator? A\n",
      "preliminary study.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "CoRR, abs/2303.04048. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\n",
      "Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. CoRR, abs/2305.17926. Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna\n",
      "Garimella, Varun Manjunatha, and Mohit Iyyer. 2023c. KNN-LM does not improve open-ended text\n",
      "generation. CoRR, abs/2305.14625.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2021. Bartscore: Evaluating generated text as text genera-\n",
      "tion. In Advances in Neural Information Processing\n",
      "Systems 34: Annual Conference on Neural Informa-\n",
      "tion Processing Systems 2021, NeurIPS 2021, De-\n",
      "cember 6-14, 2021, virtual, pages 27263–27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\n",
      "Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\n",
      "Danny Fox, Helen Meng, and James R.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "Glass. 2023. Interpretable unified language checking. CoRR,\n",
      "abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\n",
      "ating text generation with BERT. In8th International\n",
      "Conference on Learning Representations, ICLR 2020,\n",
      "Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\n",
      "view.net. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\n",
      "tian M. Meyer, and Steffen Eger.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n",
      "input_text:  \n",
      "You are a top-tier algorithm designed for extracting\n",
      "information in structured formats to build a knowledge graph.\n",
      "\n",
      "Extract the entities (nodes) and specify their type from the following text.\n",
      "Also extract the relationships between these nodes.\n",
      "\n",
      "Return result as JSON using the following format:\n",
      "{\"nodes\": [ {\"id\": \"0\", \"label\": \"Person\", \"properties\": {\"name\": \"John\"} }],\n",
      "\"relationships\": [{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {\"since\": \"2024-08-01\"} }] }\n",
      "\n",
      "Use only the following node and relationship types (if provided):\n",
      "{'node_types': (), 'relationship_types': (), 'patterns': (), 'constraints': (), 'additional_node_types': True, 'additional_relationship_types': True, 'additional_patterns': True}\n",
      "\n",
      "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
      "Do respect the source and target node types for relationship and\n",
      "the relationship direction.\n",
      "\n",
      "Make sure you adhere to the following rules to produce valid JSON objects:\n",
      "- Do not return any additional information other than the JSON in it.\n",
      "- Omit any backticks around the JSON - simply output the JSON on its own.\n",
      "- The JSON object must not wrapped into a list - it is its own JSON object.\n",
      "- Property names must be enclosed in double quotes\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input text:\n",
      "\n",
      "2019. MoverScore:\n",
      "Text generation evaluating with contextualized em-\n",
      "beddings and earth mover distance. In Proceedings\n",
      "of the 2019 Conference on Empirical Methods in\n",
      "Natural Language Processing and the 9th Interna-\n",
      "tional Joint Conference on Natural Language Pro-\n",
      "cessing (EMNLP-IJCNLP), pages 563–578, Hong\n",
      "Kong, China. Association for Computational Lin-\n",
      "guistics. A Examples from WikiEval\n",
      "Tables 2, 3 and 4 show examples from the WikiEval\n",
      "dataset, focusing in particular on answers with high\n",
      "and low faithfulness (Table 2), high and low answer\n",
      "relevance (Table 3), and high and low context rele-\n",
      "vance (Table 4). Question Context Answer\n",
      "Who directed the film Op-\n",
      "penheimer and who stars\n",
      "as J. Robert Oppenheimer\n",
      "in the film? Oppenheimer is a 2023 biographical thriller film written\n",
      "and directed by Christopher Nolan. Based on the 2005\n",
      "biography American Prometheus by Kai Bird and Mar-\n",
      "tin J. Sherwin, the film chronicles the life of J. Robert\n",
      "Oppenheimer, a theoretical physicist who was pivotal in\n",
      "developing the first nuclear weapons as part of the Man-\n",
      "hattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt\n",
      "as Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer. High Faithfulness : Christopher\n",
      "Nolan directed the film Oppen-\n",
      "heimer. Cillian Murphy stars as J. Robert Oppenheimer in the film. Low Faithfulness : James\n",
      "Cameron directed the film Op-\n",
      "penheimer. Tom Cruise stars as J. Robert Oppenheimer in the film. Table 2: Example from WikiEval, showing answers with high and low faithfulness. Question Answer\n",
      "When is the scheduled\n",
      "launch date and time for\n",
      "the PSLV-C56 mission,\n",
      "and where will it be\n",
      "launched from? High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\n",
      "July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\n",
      "Centre, Sriharikota, Andhra Pradesh, India. Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have\n",
      "not been provided.The PSLV-C56 mission is an important space mission for India. It aims to\n",
      "launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers with high and low answer relevance. Question Context\n",
      "When was the Chimnabai\n",
      "Clock Tower completed,\n",
      "and who was it named af-\n",
      "ter? High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\n",
      "a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\n",
      "in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\n",
      "Sayajirao Gaekwad III of Baroda State. Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\n",
      "a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\n",
      "in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\n",
      "Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style. History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\n",
      "I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\n",
      "inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\n",
      "Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\n",
      "of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023). Table 4: Example from WikiEval, showing answers with high and low context relevance.\n",
      "\n",
      "args:  ()\n",
      "kwargs:  {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineResult(run_id='d1a98c51-b2ba-4d82-b99a-9b20a4c9d1e0', result={'resolver': {'number_of_nodes_to_resolve': 271, 'number_of_created_nodes': 223}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await kg_pipeline.run_async(file_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KG Pipeline Tracker:\n",
    "## run_async() take the content of the page  and  call runner on it\n",
    "## FixedSizeSplitter to split the page into chunks (character count)\n",
    "## TextChunkEmbedder to create embedding of each chunk\n",
    "## SchemaFromTextExtractor to create schema with LLM\n",
    "### prompt for this :\"\"You are a top-tier algorithm designed for extracting a labeled property graph schema in structured formats......\n",
    "##LLMEntityRelationExtractor : extract the relationships and nodes from the text, it uses the only types and labels from the graph schema that SchemaFromTextExtractor provides\n",
    "## GraphPruning to compare every extracted node label and relationship type against the GraphSchema\n",
    "### It identifies relationships where the start_node or end_node is missing from the list of extracted nodes.\n",
    "## KGWriter : write the nodes and relationships to the neo4j\n",
    "##SinglePropertyExactMatchResolver : Resolve entities with same label and exact same property (default is \"name\").\n",
    "### LLM might create with different id but same properties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b5e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "create_vector_index(\n",
    "    driver, \n",
    "    name=\"chunk_vector\", \n",
    "    label=\"Chunk\", \n",
    "    embedding_property=\"embedding\", \n",
    "    dimensions=384,  \n",
    "    similarity_fn=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfbd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"chunk_vector\",\n",
    "    embedder=neo4j_embedder,\n",
    "    retrieval_query=\"\"\"\n",
    "    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\n",
    "    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\n",
    "    RETURN node.text AS text, \n",
    "           node.score AS score, \n",
    "           {related_info: info} AS metadata\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1cdafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':NEXT_CHUNK|FROM_CHUNK' instead)} {position: line: 2, column: 30, offset: 179} for query: 'CALL db.index.vector.queryNodes($vector_index_name, $top_k * $effective_search_ratio, $query_vector) YIELD node, score WITH node, score LIMIT $top_k \\n    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\\n    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\\n    RETURN node.text AS text, \\n           node.score AS score, \\n           {related_info: info} AS metadata\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: score)} {position: line: 5, column: 17, offset: 343} for query: 'CALL db.index.vector.queryNodes($vector_index_name, $top_k * $effective_search_ratio, $query_vector) YIELD node, score WITH node, score LIMIT $top_k \\n    MATCH (node)-[:NEXT_CHUNK|:FROM_CHUNK]-(neighbor)\\n    WITH node, collect(coalesce(neighbor.text, neighbor.name, labels(neighbor)[0])) AS info\\n    RETURN node.text AS text, \\n           node.score AS score, \\n           {related_info: info} AS metadata\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Context:\n",
      "<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.' score=None metadata={'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', 'Luis Espinosa-Anke', 'GPT-4', '2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.', 'AMPLYFI', 'Steven Schockaert', 'Exploding Gradients', 'Shahul Es']}>\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Question:\n",
      "What are the core components of the RAG pipeline mentioned in the document?\n",
      "\n",
      "Answer:\n",
      "\n",
      "retrieve result {'items': [{'content': \"<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\\\n†Exploding Gradients\\\\n∗CardiffNLP, Cardiff Universi... (23235 chars)\", 'metadata': {'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', '... (7 items)']}}], 'metadata': {'query_vector': [-0.10571019351482391, 0.07304587215185165, 0.04226483777165413, -0.007508380804210901, -0.050810493528842926, '... (379 items)'], '__retriever': 'VectorCypherRetriever'}}\n",
      "input_text:  Context:\n",
      "<Record text='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs. 1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs. While the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used. To address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas. arXiv:2309.15217v2  [cs.CL]  28 Apr 2025\\nof retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow. 2 Related Work\\nEstimating faithfulness using LLMs The prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible. Yet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API. For models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers. Automated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM. This idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b). In terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer). 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,\\nwe usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevance refers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023). We now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2. Faithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer. question: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format. statement: [statement 1]\\n... statement: [statement n]\\nThe final faithfulness score, F , is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements. Answer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer. answer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion. Context relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the\\ninclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context. The context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context. 2. The question should be framed from\\na part that contains non-trivial informa-\\ntion. 3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments. links. 4. The question should be of moderate\\ndifficulty. 5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans. 6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context. question: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators. Faithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context. We then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner. question: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance. Context relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for\\nFaith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context. 5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators). To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized. Given an answer and context, assign a\\nscore for faithfulness in the range 0-10. context: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question. It penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy. question: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts. 6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance. References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.' score=None metadata={'related_info': ['Ragas', 'Jithin James', 'LLM', 'CardiffNLP, Cardiff University', 'ChatGPT', 'Luis Espinosa-Anke', 'GPT-4', '2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu.', 'AMPLYFI', 'Steven Schockaert', 'Exploding Gradients', 'Shahul Es']}>\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Question:\n",
      "What are the core components of the RAG pipeline mentioned in the document?\n",
      "\n",
      "Answer:\n",
      "\n",
      "args:  (None,)\n",
      "kwargs:  {'system_instruction': 'Answer the user question using the provided context.'}\n",
      "llm_response content='{\\n\"retrieval\": \"The retrieval module\",\\n  \"generation\": \"The LLM-based generation module\"\\n}'\n",
      "Answer: {\n",
      "\"retrieval\": \"The retrieval module\",\n",
      "  \"generation\": \"The LLM-based generation module\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rag_system = GraphRAG(\n",
    "    retriever=retriever, \n",
    "    llm=llm_adapter       \n",
    ")\n",
    "\n",
    "query = \"What are the core components of the RAG pipeline mentioned in the document?\"\n",
    "response = rag_system.search(\n",
    "    query_text=query, \n",
    "    retriever_config={\"top_k\": 1}\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d5e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
