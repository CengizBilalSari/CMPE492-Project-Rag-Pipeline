{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, UnstructuredFileLoader\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "load_dotenv()\n",
    "\n",
    "uri = os.getenv(\"NEO4J_URI\")\n",
    "user = os.getenv(\"NEO4J_USERNAME\") \n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "graph = Neo4jGraph(url=uri, username=user, password=password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f9ccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents from local directory.\n",
      "\n",
      "--- FINAL PROMPT SENT TO LLM ---\n",
      " System: # Knowledge Graph Instructions for GPT-4\n",
      "## 1. Overview\n",
      "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
      "Try to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\n",
      "- **Nodes** represent entities and concepts.\n",
      "- The aim is to achieve simplicity and clarity in the knowledge graph, making it\n",
      "accessible for a vast audience.\n",
      "## 2. Labeling Nodes\n",
      "- **Consistency**: Ensure you use available types for node labels.\n",
      "Ensure you use basic or elementary types for node labels.\n",
      "- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
      "- **Relationships** represent connections between entities or concepts.\n",
      "Ensure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\n",
      "## 3. Coreference Resolution\n",
      "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
      "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
      "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
      "## 4. Strict Compliance\n",
      "Adhere to the rules strictly. Non-compliance will result in termination.\n",
      "Human:  Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Ragas: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es†, Jithin James †, Luis Espinosa-Anke ∗♢, Steven Schockaert ∗\n",
      "†Exploding Gradients\n",
      "∗CardiffNLP, Cardiff University, United Kingdom\n",
      "♢AMPLYFI, United Kingdom\n",
      "shahules786@gmail.com,jamesjithin97@gmail.com\n",
      "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "Abstract\n",
      "We introduce Ragas (Retrieval Augmented\n",
      "Generation Assessment), a framework for\n",
      "reference-free evaluation of Retrieval Aug-\n",
      "mented Generation (RAG) pipelines. RAG\n",
      "systems are composed of a retrieval and an\n",
      "LLM based generation module, and provide\n",
      "LLMs with knowledge from a reference textual\n",
      "database, which enables them to act as a natu-\n",
      "ral language layer between a user and textual\n",
      "databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal-\n",
      "lenging because there are several dimensions to\n",
      "consider: the ability of the retrieval system to\n",
      "identify relevant and focused context passages,\n",
      "the ability of the LLM to exploit such passages\n",
      "in a faithful way, or the quality of the genera-\n",
      "tion itself. With Ragas, we put forward a suite\n",
      "of metrics which can be used to evaluate these\n",
      "different dimensions without having to rely on\n",
      "ground truth human annotations. We posit that\n",
      "such a framework can crucially contribute to\n",
      "faster evaluation cycles of RAG architectures,\n",
      "which is especially important given the fast\n",
      "adoption of LLMs.\n",
      "1 Introduction\n",
      "Language Models (LMs) capture a vast amount\n",
      "of knowledge about the world, which allows them\n",
      "to answer questions without accessing any exter-\n",
      "nal sources. This idea of LMs as repositories of\n",
      "knowledge emerged shortly after the introduction\n",
      "of BERT (Devlin et al., 2019) and became more\n",
      "firmly established with the introduction of ever\n",
      "larger LMs (Roberts et al., 2020). While the most\n",
      "recent Large Language Models (LLMs) capture\n",
      "enough knowledge to rival human performance\n",
      "across a wide variety of question answering bench-\n",
      "marks (Bubeck et al., 2023), the idea of using\n",
      "LLMs as knowledge bases still has two fundamen-\n",
      "tal limitations. First, LLMs are not able to answer\n",
      "questions about events that have happened after\n",
      "they were trained. Second, even the largest models\n",
      "struggle to memorise knowledge that is only rarely\n",
      "mentioned in the training corpus (Kandpal et al.,\n",
      "2022; Mallen et al., 2023). The standard solution\n",
      "to these issues is to rely on Retrieval Augmented\n",
      "Generation (RAG) (Lee et al., 2019; Lewis et al.,\n",
      "2020; Guu et al., 2020). Answering a question\n",
      "then essentially involves retrieving relevant pas-\n",
      "sages from a corpus and feeding these passages,\n",
      "along with the original question, to the LM. While\n",
      "initial approaches relied on specialised LMs for\n",
      "retrieval-augmented language modelling (Khandel-\n",
      "wal et al., 2020; Borgeaud et al., 2022), recent work\n",
      "has suggested that simply adding retrieved docu-\n",
      "ments to the input of a standard LM can also work\n",
      "well (Khattab et al., 2022; Ram et al., 2023; Shi\n",
      "et al., 2023), thus making it possible to use retrieval-\n",
      "augmented strategies in combination with LLMs\n",
      "that are only available through APIs.\n",
      "While the usefulness of retrieval-augmented\n",
      "strategies is clear, their implementation requires\n",
      "a significant amount of tuning, as the overall per-\n",
      "formance will be affected by the retrieval model,\n",
      "the considered corpus, the LM, or the prompt for-\n",
      "mulation, among others. Automated evaluation of\n",
      "retrieval-augmented systems is thus paramount. In\n",
      "practice, RAG systems are often evaluated in terms\n",
      "of the language modelling task itself, i.e. by mea-\n",
      "suring perplexity on some reference corpus. How-\n",
      "ever, such evaluations are not always predictive\n",
      "of downstream performance (Wang et al., 2023c).\n",
      "Moreover, this evaluation strategy relies on the LM\n",
      "probabilities, which are not accessible for some\n",
      "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
      "tion answering is another common evaluation task,\n",
      "but usually only datasets with short extractive an-\n",
      "swers are considered, which may not be represen-\n",
      "tative of how the system will be used.\n",
      "To address these issues, in this paper we present\n",
      "Ragas1, a framework for the automated assessment\n",
      "1Ragas is available at https://github.com/\n",
      "explodinggradients/ragas.\n",
      "arXiv:2309.15217v2  [cs.CL]  28 Apr 2025 \n",
      "==============================\n",
      "None\n",
      "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '0vhxmqddr', 'function': {'arguments': '{\"nodes\":[{\"id\":\"Ragas\",\"type\":\"framework\"},{\"id\":\"Retrieval Augmented Generation\",\"type\":\"pipeline\"},{\"id\":\"BERT\",\"type\":\"language model\"},{\"id\":\"LLMs\",\"type\":\"language model\"},{\"id\":\"CardiffNLP\",\"type\":\"organization\"},{\"id\":\"Cardiff University\",\"type\":\"organization\"},{\"id\":\"AMPLYFI\",\"type\":\"organization\"},{\"id\":\"Shahul Es†\",\"type\":\"person\"},{\"id\":\"Jithin James †\",\"type\":\"person\"},{\"id\":\"Luis Espinosa-Anke ∗♢\",\"type\":\"person\"},{\"id\":\"Steven Schockaert ∗\",\"type\":\"person\"}],\"relationships\":[{\"source_node_id\":\"Ragas\",\"source_node_type\":\"framework\",\"target_node_id\":\"Retrieval Augmented Generation\",\"target_node_type\":\"pipeline\",\"type\":\"assesses\"},{\"source_node_id\":\"Retrieval Augmented Generation\",\"source_node_type\":\"pipeline\",\"target_node_id\":\"BERT\",\"target_node_type\":\"language model\",\"type\":\"uses\"},{\"source_node_id\":\"Retrieval Augmented Generation\",\"source_node_type\":\"pipeline\",\"target_node_id\":\"LLMs\",\"target_node_type\":\"language model\",\"type\":\"uses\"},{\"source_node_id\":\"CardiffNLP\",\"source_node_type\":\"organization\",\"target_node_id\":\"Shahul Es†\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"CardiffNLP\",\"source_node_type\":\"organization\",\"target_node_id\":\"Luis Espinosa-Anke ∗♢\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"CardiffNLP\",\"source_node_type\":\"organization\",\"target_node_id\":\"Steven Schockaert ∗\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"Cardiff University\",\"source_node_type\":\"organization\",\"target_node_id\":\"Shahul Es†\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"Cardiff University\",\"source_node_type\":\"organization\",\"target_node_id\":\"Luis Espinosa-Anke ∗♢\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"Cardiff University\",\"source_node_type\":\"organization\",\"target_node_id\":\"Steven Schockaert ∗\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"},{\"source_node_id\":\"AMPLYFI\",\"source_node_type\":\"organization\",\"target_node_id\":\"Luis Espinosa-Anke ∗♢\",\"target_node_type\":\"person\",\"type\":\"affiliated with\"}]}', 'name': 'DynamicGraph'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 662, 'prompt_tokens': 2136, 'total_tokens': 2798, 'completion_time': 0.796786878, 'completion_tokens_details': None, 'prompt_time': 0.170327593, 'prompt_tokens_details': None, 'queue_time': 0.092162488, 'total_time': 0.967114471}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bf8fe-97a9-7fa2-bd39-2891d56b5fa3-0', tool_calls=[{'name': 'DynamicGraph', 'args': {'nodes': [{'id': 'Ragas', 'type': 'framework'}, {'id': 'Retrieval Augmented Generation', 'type': 'pipeline'}, {'id': 'BERT', 'type': 'language model'}, {'id': 'LLMs', 'type': 'language model'}, {'id': 'CardiffNLP', 'type': 'organization'}, {'id': 'Cardiff University', 'type': 'organization'}, {'id': 'AMPLYFI', 'type': 'organization'}, {'id': 'Shahul Es†', 'type': 'person'}, {'id': 'Jithin James †', 'type': 'person'}, {'id': 'Luis Espinosa-Anke ∗♢', 'type': 'person'}, {'id': 'Steven Schockaert ∗', 'type': 'person'}], 'relationships': [{'source_node_id': 'Ragas', 'source_node_type': 'framework', 'target_node_id': 'Retrieval Augmented Generation', 'target_node_type': 'pipeline', 'type': 'assesses'}, {'source_node_id': 'Retrieval Augmented Generation', 'source_node_type': 'pipeline', 'target_node_id': 'BERT', 'target_node_type': 'language model', 'type': 'uses'}, {'source_node_id': 'Retrieval Augmented Generation', 'source_node_type': 'pipeline', 'target_node_id': 'LLMs', 'target_node_type': 'language model', 'type': 'uses'}, {'source_node_id': 'CardiffNLP', 'source_node_type': 'organization', 'target_node_id': 'Shahul Es†', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'CardiffNLP', 'source_node_type': 'organization', 'target_node_id': 'Luis Espinosa-Anke ∗♢', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'CardiffNLP', 'source_node_type': 'organization', 'target_node_id': 'Steven Schockaert ∗', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'Cardiff University', 'source_node_type': 'organization', 'target_node_id': 'Shahul Es†', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'Cardiff University', 'source_node_type': 'organization', 'target_node_id': 'Luis Espinosa-Anke ∗♢', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'Cardiff University', 'source_node_type': 'organization', 'target_node_id': 'Steven Schockaert ∗', 'target_node_type': 'person', 'type': 'affiliated with'}, {'source_node_id': 'AMPLYFI', 'source_node_type': 'organization', 'target_node_id': 'Luis Espinosa-Anke ∗♢', 'target_node_type': 'person', 'type': 'affiliated with'}]}, 'id': '0vhxmqddr', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 2136, 'output_tokens': 662, 'total_tokens': 2798}), 'parsed': DynamicGraph(nodes=[SimpleNode(id='Ragas', type='framework'), SimpleNode(id='Retrieval Augmented Generation', type='pipeline'), SimpleNode(id='BERT', type='language model'), SimpleNode(id='LLMs', type='language model'), SimpleNode(id='CardiffNLP', type='organization'), SimpleNode(id='Cardiff University', type='organization'), SimpleNode(id='AMPLYFI', type='organization'), SimpleNode(id='Shahul Es†', type='person'), SimpleNode(id='Jithin James †', type='person'), SimpleNode(id='Luis Espinosa-Anke ∗♢', type='person'), SimpleNode(id='Steven Schockaert ∗', type='person')], relationships=[SimpleRelationship(source_node_id='Ragas', source_node_type='framework', target_node_id='Retrieval Augmented Generation', target_node_type='pipeline', type='assesses'), SimpleRelationship(source_node_id='Retrieval Augmented Generation', source_node_type='pipeline', target_node_id='BERT', target_node_type='language model', type='uses'), SimpleRelationship(source_node_id='Retrieval Augmented Generation', source_node_type='pipeline', target_node_id='LLMs', target_node_type='language model', type='uses'), SimpleRelationship(source_node_id='CardiffNLP', source_node_type='organization', target_node_id='Shahul Es†', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='CardiffNLP', source_node_type='organization', target_node_id='Luis Espinosa-Anke ∗♢', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='CardiffNLP', source_node_type='organization', target_node_id='Steven Schockaert ∗', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='Cardiff University', source_node_type='organization', target_node_id='Shahul Es†', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='Cardiff University', source_node_type='organization', target_node_id='Luis Espinosa-Anke ∗♢', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='Cardiff University', source_node_type='organization', target_node_id='Steven Schockaert ∗', target_node_type='person', type='affiliated with'), SimpleRelationship(source_node_id='AMPLYFI', source_node_type='organization', target_node_id='Luis Espinosa-Anke ∗♢', target_node_type='person', type='affiliated with')]), 'parsing_error': None}\n",
      "\n",
      "--- FINAL PROMPT SENT TO LLM ---\n",
      " System: # Knowledge Graph Instructions for GPT-4\n",
      "## 1. Overview\n",
      "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
      "Try to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\n",
      "- **Nodes** represent entities and concepts.\n",
      "- The aim is to achieve simplicity and clarity in the knowledge graph, making it\n",
      "accessible for a vast audience.\n",
      "## 2. Labeling Nodes\n",
      "- **Consistency**: Ensure you use available types for node labels.\n",
      "Ensure you use basic or elementary types for node labels.\n",
      "- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
      "- **Relationships** represent connections between entities or concepts.\n",
      "Ensure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\n",
      "## 3. Coreference Resolution\n",
      "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
      "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
      "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
      "## 4. Strict Compliance\n",
      "Adhere to the rules strictly. Non-compliance will result in termination.\n",
      "Human:  Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: of retrieval augmented generation systems. We fo-\n",
      "cus on settings where reference answers may not be\n",
      "available, and where we want to estimate different\n",
      "proxies for correctness, in addition to the useful-\n",
      "ness of the retrieved passages. The Ragas frame-\n",
      "work provides an integration with both llama-index\n",
      "and Langchain, the most widely used frameworks\n",
      "for building RAG solutions, thus enabling devel-\n",
      "opers to easily integrate Ragas into their standard\n",
      "workflow.\n",
      "2 Related Work\n",
      "Estimating faithfulness using LLMs The prob-\n",
      "lem of detecting hallucinations in LLM generated\n",
      "responses has been extensively studied (Ji et al.,\n",
      "2023). Several authors have suggested the idea\n",
      "of predicting factuality using a few-shot prompt-\n",
      "ing strategy (Zhang et al., 2023). Recent analy-\n",
      "ses, however, suggest that existing models struggle\n",
      "with detecting hallucination when using standard\n",
      "prompting strategies (Li et al., 2023; Azaria and\n",
      "Mitchell, 2023). Other approaches rely on linking\n",
      "the generated responses to facts from an external\n",
      "knowledge base (Min et al., 2023), but this is not\n",
      "always possible.\n",
      "Yet another strategy is to inspect the probabili-\n",
      "ties assigned to individual tokens, where we would\n",
      "expect the model to be less confident in halluci-\n",
      "nated answers than in factual ones. For instance,\n",
      "BARTScore (Yuan et al., 2021) estimates factuality\n",
      "by looking at the conditional probability of the gen-\n",
      "erated text given the input. Kadavath et al. (2022)\n",
      "use a variation of this idea. Starting from the ob-\n",
      "servation that LLMs provide well-calibrated proba-\n",
      "bilities when answering multiple-choice questions,\n",
      "they essentially convert the problem of validating\n",
      "model generated answers into a multiple-choice\n",
      "question which asks whether the answer is true or\n",
      "false. Rather than looking at the output probabil-\n",
      "ities, Azaria and Mitchell (2023) propose to train\n",
      "a supervised classifier on the weights from one of\n",
      "the hidden layers of the LLM, to predict whether a\n",
      "given statement is true or not. While the approach\n",
      "performs well, the need to access the hidden states\n",
      "of the model makes it unsuitable for systems that\n",
      "access LLMs through an API.\n",
      "For models that do not provide access to token\n",
      "probabilities, such as ChatGPT and GPT-4, differ-\n",
      "ent methods are needed. SelfCheckGPT (Manakul\n",
      "et al., 2023) addresses this problem by instead sam-\n",
      "pling multiple answers. Their core idea is that\n",
      "factual answers are more stable: when an answer is\n",
      "factual, we can expect that different samples will\n",
      "tend to be semantically similar, whereas this is less\n",
      "likely to be the case for hallucinated answers.\n",
      "Automated evaluation of text generation systems\n",
      "LLMs have also been leveraged to automatically\n",
      "evaluate other aspects of generated text fragments,\n",
      "beyond factuality. For instance, GPTScore (Fu\n",
      "et al., 2023) uses a prompt that specifies the consid-\n",
      "ered aspect (e.g. fluency) and then scores passages\n",
      "based on the average probability of the generated\n",
      "tokens, according to a given autoregressive LM.\n",
      "This idea of using prompts was previously also\n",
      "considered by Yuan et al. (2021), although they\n",
      "used a smaller fine-tuned LM (i.e. BART) and did\n",
      "not observe a clear benefit from using prompts. An-\n",
      "other approach directly asks ChatGPT to evaluate\n",
      "a particular aspect of the given answer by provid-\n",
      "ing a score between 0 and 100, or by providing a\n",
      "rating on a 5-star scale (Wang et al., 2023a). Re-\n",
      "markably, strong results can be obtained in this\n",
      "way, although it comes with the limitation of being\n",
      "sensitive to the design of the prompt. Rather than\n",
      "scoring individual answers, some authors have also\n",
      "focused on using an LLM to select the best answer\n",
      "among a number of candidates (Wang et al., 2023b),\n",
      "typically to compare the performance of different\n",
      "LLMs. However, care is needed with this approach,\n",
      "as the order in which the answers is presented can\n",
      "influence the result (Wang et al., 2023b).\n",
      "In terms of how ground truth answers or, more\n",
      "generally, generations, have been typically used\n",
      "in the literature, most approaches have relied on\n",
      "the availability of one or more reference answers.\n",
      "For instance, BERTScore (Zhang et al., 2020)\n",
      "and MoverScore (Zhao et al., 2019) use contex-\n",
      "tualised embeddings, produced by a pre-trained\n",
      "BERT model, to compare the similarity between\n",
      "the generated answer and the reference answers.\n",
      "BARTScore (Yuan et al., 2021) similarly uses refer-\n",
      "ence answers to compute aspects such as precision\n",
      "(estimated as the probability of generating the gen-\n",
      "erated answer given the reference) and recall (esti-\n",
      "mated as the probability of generating the reference\n",
      "given the generated answer).\n",
      "3 Evaluation Strategies\n",
      "We consider a standard RAG setting, where given a\n",
      "question q, the system first retrieves some context\n",
      "c(q) and then uses the retrieved context to generate\n",
      "an answer as(q). When building a RAG system, \n",
      "==============================\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=DynamicGraph> {\"nodes\": [{\"id\": \"Ragas\", \"type\": \"framework\"}, {\"id\": \"llama-index\", \"type\": \"framework\"}, {\"id\": \"Langchain\", \"type\": \"framework\"}, {\"id\": \"Ji et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Zhang et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Li et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Azaria and Mitchell\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Min et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Yuan et al.\", \"2021\", \"type\": \"research-paper\"}, {\"id\": \"Kadavath et al.\", \"2022\", \"type\": \"research-paper\"}, {\"id\": \"BARTScore\", \"type\": \"metric\"}, {\"id\": \"GPTScore\", \"type\": \"metric\"}, {\"id\": \"SelfCheckGPT\", \"type\": \"metric\"}, {\"id\": \"ChatGPT\", \"type\": \"LLM\"}, {\"id\": \"GPT-4\", \"type\": \"LLM\"}, {\"id\": \"BERTScore\", \"type\": \"metric\"}, {\"id\": \"MoverScore\", \"type\": \"metric\"}, {\"id\": \"BERT\", \"type\": \"model\"}, {\"id\": \"BERT model\", \"type\": \"model\"}, {\"id\": \"question\", \"type\": \"entity\"}, {\"id\": \"context\", \"type\": \"entity\"}, {\"id\": \"answer\", \"type\": \"entity\"}, {\"id\": \"RAG system\", \"type\": \"entity\"}, {\"id\": \"LLM\", \"type\": \"entity\"}, {\"id\": \"prompt\", \"type\": \"entity\"}, {\"id\": \"aspect\", \"type\": \"entity\"}, {\"id\": \"evaluation\", \"type\": \"entity\"}, {\"id\": \"metric\", \"type\": \"entity\"}, {\"id\": \"research-paper\", \"type\": \"entity\"}, {\"id\": \"framework\", \"type\": \"entity\"}, {\"id\": \"model\", \"type\": \"entity\"}, {\"id\": \"entity\", \"type\": \"entity\"}], \"relationships\": [{\"source_node_id\": \"Ragas\", \"source_node_type\": \"framework\", \"target_node_id\": \"llama-index\", \"target_node_type\": \"framework\", \"type\": \"integrates-with\"}, {\"source_node_id\": \"Ragas\", \"source_node_type\": \"framework\", \"target_node_id\": \"Langchain\", \"target_node_type\": \"framework\", \"type\": \"integrates-with\"}, {\"source_node_id\": \"Ji et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Zhang et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Li et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Azaria and Mitchell\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Min et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Yuan et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Kadavath et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"BARTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"GPTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"SelfCheckGPT\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"ChatGPT\", \"source_node_type\": \"LLM\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"GPT-4\", \"source_node_type\": \"LLM\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"BERTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"MoverScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"BERT\", \"source_node_type\": \"model\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"question\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"context\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"answer\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"output\"}, {\"source_node_id\": \"RAG system\", \"source_node_type\": \"entity\", \"target_node_id\": \"evaluation\", \"target_node_type\": \"entity\", \"type\": \"evaluated-by\"}, {\"source_node_id\": \"LLM\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"uses\"}, {\"source_node_id\": \"prompt\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"aspect\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"evaluation\", \"source_node_type\": \"entity\", \"target_node_id\": \"metric\", \"target_node_type\": \"entity\", \"type\": \"evaluated-by\"}, {\"source_node_id\": \"metric\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"evaluates\"}, {\"source_node_id\": \"research-paper\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"framework\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"model\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"entity\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}]} </function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents from local directory.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m transformer = LLMGraphTransformer(llm=llm)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m graph_documents = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_graph_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# this method creates the cypher queries for the graph_documents created by the langchain\u001b[39;00m\n\u001b[32m     21\u001b[39m graph.add_graph_documents(graph_documents, baseEntityLabel=\u001b[38;5;28;01mTrue\u001b[39;00m, include_source=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_experimental\\graph_transformers\\llm.py:936\u001b[39m, in \u001b[36mLLMGraphTransformer.convert_to_graph_documents\u001b[39m\u001b[34m(self, documents, config)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_graph_documents\u001b[39m(\n\u001b[32m    925\u001b[39m     \u001b[38;5;28mself\u001b[39m, documents: Sequence[Document], config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    926\u001b[39m ) -> List[GraphDocument]:\n\u001b[32m    927\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert a sequence of documents into graph documents.\u001b[39;00m\n\u001b[32m    928\u001b[39m \n\u001b[32m    929\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m \u001b[33;03m        Sequence[GraphDocument]: The transformed documents as graphs.\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_experimental\\graph_transformers\\llm.py:841\u001b[39m, in \u001b[36mLLMGraphTransformer.process_response\u001b[39m\u001b[34m(self, document, config)\u001b[39m\n\u001b[32m    839\u001b[39m full_prompt = \u001b[38;5;28mself\u001b[39m.chain.first.format(\u001b[38;5;28minput\u001b[39m=text)\n\u001b[32m    840\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- FINAL PROMPT SENT TO LLM ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, full_prompt, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m raw_schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[32m    843\u001b[39m \u001b[38;5;28mprint\u001b[39m(raw_schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3876\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3870\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3871\u001b[39m         futures = [\n\u001b[32m   3872\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3873\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3874\u001b[39m         ]\n\u001b[32m   3875\u001b[39m         output = {\n\u001b[32m-> \u001b[39m\u001b[32m3876\u001b[39m             key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3877\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3878\u001b[39m         }\n\u001b[32m   3879\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3859\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3853\u001b[39m child_config = patch_config(\n\u001b[32m   3854\u001b[39m     config,\n\u001b[32m   3855\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3856\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3857\u001b[39m )\n\u001b[32m   3858\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3861\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\langchain_groq\\chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Code\\venv_neo4j\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=DynamicGraph> {\"nodes\": [{\"id\": \"Ragas\", \"type\": \"framework\"}, {\"id\": \"llama-index\", \"type\": \"framework\"}, {\"id\": \"Langchain\", \"type\": \"framework\"}, {\"id\": \"Ji et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Zhang et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Li et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Azaria and Mitchell\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Min et al.\", \"2023\", \"type\": \"research-paper\"}, {\"id\": \"Yuan et al.\", \"2021\", \"type\": \"research-paper\"}, {\"id\": \"Kadavath et al.\", \"2022\", \"type\": \"research-paper\"}, {\"id\": \"BARTScore\", \"type\": \"metric\"}, {\"id\": \"GPTScore\", \"type\": \"metric\"}, {\"id\": \"SelfCheckGPT\", \"type\": \"metric\"}, {\"id\": \"ChatGPT\", \"type\": \"LLM\"}, {\"id\": \"GPT-4\", \"type\": \"LLM\"}, {\"id\": \"BERTScore\", \"type\": \"metric\"}, {\"id\": \"MoverScore\", \"type\": \"metric\"}, {\"id\": \"BERT\", \"type\": \"model\"}, {\"id\": \"BERT model\", \"type\": \"model\"}, {\"id\": \"question\", \"type\": \"entity\"}, {\"id\": \"context\", \"type\": \"entity\"}, {\"id\": \"answer\", \"type\": \"entity\"}, {\"id\": \"RAG system\", \"type\": \"entity\"}, {\"id\": \"LLM\", \"type\": \"entity\"}, {\"id\": \"prompt\", \"type\": \"entity\"}, {\"id\": \"aspect\", \"type\": \"entity\"}, {\"id\": \"evaluation\", \"type\": \"entity\"}, {\"id\": \"metric\", \"type\": \"entity\"}, {\"id\": \"research-paper\", \"type\": \"entity\"}, {\"id\": \"framework\", \"type\": \"entity\"}, {\"id\": \"model\", \"type\": \"entity\"}, {\"id\": \"entity\", \"type\": \"entity\"}], \"relationships\": [{\"source_node_id\": \"Ragas\", \"source_node_type\": \"framework\", \"target_node_id\": \"llama-index\", \"target_node_type\": \"framework\", \"type\": \"integrates-with\"}, {\"source_node_id\": \"Ragas\", \"source_node_type\": \"framework\", \"target_node_id\": \"Langchain\", \"target_node_type\": \"framework\", \"type\": \"integrates-with\"}, {\"source_node_id\": \"Ji et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Zhang et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Li et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Azaria and Mitchell\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Min et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Yuan et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"Kadavath et al.\", \"source_node_type\": \"research-paper\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"cited-by\"}, {\"source_node_id\": \"BARTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"GPTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"SelfCheckGPT\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"ChatGPT\", \"source_node_type\": \"LLM\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"GPT-4\", \"source_node_type\": \"LLM\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"BERTScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"MoverScore\", \"source_node_type\": \"metric\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"evaluates\"}, {\"source_node_id\": \"BERT\", \"source_node_type\": \"model\", \"target_node_id\": \"Ragas\", \"target_node_type\": \"framework\", \"type\": \"uses\"}, {\"source_node_id\": \"question\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"context\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"answer\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"output\"}, {\"source_node_id\": \"RAG system\", \"source_node_type\": \"entity\", \"target_node_id\": \"evaluation\", \"target_node_type\": \"entity\", \"type\": \"evaluated-by\"}, {\"source_node_id\": \"LLM\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"uses\"}, {\"source_node_id\": \"prompt\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"aspect\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"input\"}, {\"source_node_id\": \"evaluation\", \"source_node_type\": \"entity\", \"target_node_id\": \"metric\", \"target_node_type\": \"entity\", \"type\": \"evaluated-by\"}, {\"source_node_id\": \"metric\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"evaluates\"}, {\"source_node_id\": \"research-paper\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"framework\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"model\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}, {\"source_node_id\": \"entity\", \"source_node_type\": \"entity\", \"target_node_id\": \"RAG system\", \"target_node_type\": \"entity\", \"type\": \"related-to\"}]} </function>'}}"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n",
    "path = r\"C:\\Users\\Cengizhan\\Desktop\\CMPE492-Project-Rag-Pipeline\\Documents\\Ragas\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path, \n",
    "    glob=\"**/*.pdf\", \n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents from local directory.\")\n",
    "\n",
    "transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = transformer.convert_to_graph_documents(docs)\n",
    "\n",
    "# this method creates the cypher queries for the graph_documents created by the langchain\n",
    "graph.add_graph_documents(graph_documents, baseEntityLabel=True, include_source=True)\n",
    "\n",
    "print(\"Knowledge Graph successfully built in Neo4j using Groq.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6643cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6042b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
